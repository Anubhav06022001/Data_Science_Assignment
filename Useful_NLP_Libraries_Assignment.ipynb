{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful NLP Libraries and Networks Assignment\n",
    "\n",
    "## Theory Questions\n",
    "### 1. What is NLTK?\n",
    "NLTK (Natural Language Toolkit) is a popular Python library for natural language processing. It provides tools for tokenization, lemmatization, stemming, parsing, classification, and other NLP tasks, making it a versatile toolkit for researchers and developers.\n",
    "\n",
    "### 2. What is SpaCy and how does it differ from NLTK?\n",
    "SpaCy is a modern NLP library designed for speed and efficiency. Unlike NLTK, which focuses on education and research, SpaCy is built for industrial applications, offering fast tokenization, named entity recognition (NER), dependency parsing, and word vectors.\n",
    "\n",
    "### 3. What is the purpose of TextBlob in NLP?\n",
    "TextBlob is a simple NLP library that provides easy-to-use tools for common text processing tasks like sentiment analysis, tokenization, noun phrase extraction, and text classification. It is often used for prototyping and quick text analysis.\n",
    "\n",
    "### 4. What is Stanford NLP?\n",
    "Stanford NLP is a suite of NLP tools developed by the Stanford NLP Group. It includes tools for tasks like tokenization, part-of-speech tagging, named entity recognition, and dependency parsing, and is known for its accuracy and robustness.\n",
    "\n",
    "### 5. Explain what Recurrent Neural Networks (RNN) are.\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed for processing sequential data. They have a feedback loop that allows them to maintain a hidden state, making them well-suited for tasks involving time-series or natural language.\n",
    "\n",
    "### 6. What is the main advantage of using LSTM over RNN?\n",
    "The main advantage of using Long Short-Term Memory (LSTM) networks over RNNs is their ability to handle long-term dependencies and avoid the vanishing gradient problem. LSTMs use specialized gate mechanisms to retain important information over long sequences.\n",
    "\n",
    "### 7. What are Bi-directional LSTMs, and how do they differ from standard LSTMs?\n",
    "Bi-directional LSTMs are a type of LSTM that process input sequences in both forward and backward directions, allowing them to capture context from both past and future states. Standard LSTMs process sequences in only one direction.\n",
    "\n",
    "### 8. What is the purpose of a Stacked LSTM?\n",
    "A Stacked LSTM consists of multiple LSTM layers stacked on top of each other. This architecture allows the model to learn more complex features and capture higher-level representations of the data, leading to better performance on challenging tasks.\n",
    "\n",
    "### 9. How does a GRU (Gated Recurrent Unit) differ from an LSTM?\n",
    "A GRU (Gated Recurrent Unit) is a simplified version of an LSTM. It has fewer gate mechanisms, combining the forget and input gates into a single update gate, which makes it computationally efficient while retaining the ability to handle long-term dependencies.\n",
    "\n",
    "### 10. What are the key features of NLTK's tokenization process?\n",
    "NLTK's tokenization process includes splitting text into sentences or words, handling punctuation, and providing different tokenization options like `word_tokenize()` for word-level tokenization and `sent_tokenize()` for sentence-level tokenization.\n",
    "\n",
    "### 11. How do you perform named entity recognition (NER) using SpaCy?\n",
    "To perform NER using SpaCy, we load a pre-trained SpaCy model (e.g., `en_core_web_sm`), pass the text to the model, and then extract named entities using the `ents` attribute of the `Doc` object.\n",
    "\n",
    "### 12. What is Word2Vec and how does it represent words?\n",
    "Word2Vec is a neural network-based model that represents words as dense vectors in a continuous vector space. It captures semantic relationships between words, where similar words are closer together in the vector space.\n",
    "\n",
    "### 13. Explain the difference between Bag of Words (BoW) and Word2Vec.\n",
    "- **Bag of Words (BoW)**: Represents text as a vector of word counts or occurrences, ignoring word order and context.\n",
    "- **Word2Vec**: Represents words as dense vectors that capture semantic relationships, preserving the meaning and context of words.\n",
    "\n",
    "### 14. How does TextBlob handle sentiment analysis?\n",
    "TextBlob handles sentiment analysis by using a predefined lexicon to determine the polarity (positive, negative, neutral) and subjectivity of a given text. It provides a simple interface to analyze the sentiment of sentences or documents.\n",
    "\n",
    "### 15. How would you implement text preprocessing using NLTK?\n",
    "Text preprocessing using NLTK involves tokenization (`word_tokenize`), removing stopwords (`stopwords`), stemming (`PorterStemmer`), lemmatization (`WordNetLemmatizer`), and text normalization (lowercasing, removing punctuation).\n",
    "\n",
    "### 16. How do you train a custom NER model using SpaCy?\n",
    "To train a custom NER model using SpaCy, you need labeled training data, create a blank SpaCy model or load a pre-trained one, and use SpaCy's `EntityRecognizer` to add new labels and train the model with the labeled data.\n",
    "\n",
    "### 17. What is the role of the attention mechanism in LSTMs and GRUs?\n",
    "The attention mechanism allows LSTMs and GRUs to focus on specific parts of the input sequence when making predictions, improving performance in tasks like machine translation and text summarization by capturing relevant information from longer sequences.\n",
    "\n",
    "### 18. What is the difference between tokenization and lemmatization in NLP?\n",
    "- **Tokenization**: The process of splitting text into individual units (words or sentences).\n",
    "- **Lemmatization**: The process of reducing words to their base or dictionary form (lemma), ensuring consistent representation of words.\n",
    "\n",
    "### 19. How do you perform text normalization in NLP?\n",
    "Text normalization in NLP involves converting text to a standard format by lowercasing, removing punctuation, expanding contractions, removing special characters, and standardizing word forms using lemmatization or stemming.\n",
    "\n",
    "### 20. What is the purpose of frequency distribution in NLP?\n",
    "Frequency distribution helps analyze how often each word appears in a text, providing insights into the importance of specific terms, identifying stopwords, and determining key topics or themes in the text.\n",
    "\n",
    "### 21. What are co-occurrence vectors in NLP?\n",
    "Co-occurrence vectors represent words based on their occurrence with other words in a given context. They capture the relationships between words, showing how often words appear together within a specific window.\n",
    "\n",
    "### 22. How is Word2Vec used to find the relationship between words?\n",
    "Word2Vec represents words as vectors, and the relationships between words can be found by calculating the similarity between their vectors using cosine similarity or other distance measures. Similar words will have similar vectors.\n",
    "\n",
    "### 23. How does a Bi-LSTM improve NLP tasks compared to a regular LSTM?\n",
    "A Bi-LSTM processes input sequences in both forward and backward directions, allowing it to capture context from both past and future words, leading to improved performance in tasks like text classification and named entity recognition.\n",
    "\n",
    "### 24. What is the difference between a GRU and an LSTM in terms of gate structures?\n",
    "A GRU has two gates (update and reset), while an LSTM has three gates (input, forget, and output). GRUs are simpler and computationally efficient, whereas LSTMs are more complex and better at capturing long-term dependencies.\n",
    "\n",
    "### 25. How does Stanford NLPâ€™s dependency parsing work?\n",
    "Stanford NLP's dependency parsing analyzes the grammatical structure of a sentence, identifying relationships between words (e.g., subject, object) and creating a tree structure that represents syntactic dependencies.\n",
    "\n",
    "### 26. How does tokenization affect downstream NLP tasks?\n",
    "Tokenization affects downstream NLP tasks by breaking text into manageable units (words or sentences) that can be analyzed. Proper tokenization ensures that models can accurately understand and learn from the text.\n",
    "\n",
    "### 27. What are some common applications of NLP?\n",
    "Common applications of NLP include chatbots, machine translation, sentiment analysis, text summarization, speech recognition, and named entity recognition.\n",
    "\n",
    "### 28. What are stopwords and why are they removed in NLP?\n",
    "Stopwords are common words (e.g., 'the', 'is', 'and') that do not carry significant meaning. They are removed in NLP to reduce noise and focus on the more meaningful words in the text.\n",
    "\n",
    "### 29. How can you implement word embeddings using Word2Vec in Python?\n",
    "You can implement word embeddings using Word2Vec in Python with the Gensim library. First, tokenize the text, then train a Word2Vec model using `gensim.models.Word2Vec()` to obtain word vectors.\n",
    "\n",
    "### 30. How does SpaCy handle lemmatization?\n",
    "SpaCy handles lemmatization by using its built-in `lemmatizer` to reduce words to their base forms. You can access the lemma of a token using the `lemma_` attribute of the `Token`.\n",
    "\n",
    "### 31. What is the significance of RNNs in NLP tasks?\n",
    "Recurrent Neural Networks (RNNs) are crucial in Natural Language Processing (NLP) tasks because they are designed to handle sequential data. Unlike traditional neural networks, RNNs have connections that loop back on themselves, allowing them to maintain a memory of previous inputs in the sequence. This makes RNNs well-suited for tasks such as text generation, sentiment analysis, machine translation, and speech recognition, where the order of words or tokens is important.\n",
    "\n",
    "### 32. How does word embedding improve the performance of NLP models?\n",
    "Word embeddings are a type of representation that translates words into dense vectors of real numbers, capturing semantic meaning. By mapping words with similar meanings to similar vectors, embeddings allow models to generalize better by understanding relationships between words (e.g., \"king\" and \"queen\" are closer in vector space). This reduces the sparsity of one-hot encoding and helps NLP models, such as neural networks, to process text more effectively, improving performance on tasks like classification, translation, and sentiment analysis.\n",
    "\n",
    "### 33. How does a Stacked LSTM differ from a single LSTM?\n",
    "A Stacked LSTM consists of multiple LSTM layers stacked on top of one another, with each layer feeding its output into the next. This structure allows the model to learn more complex hierarchical features from the data compared to a single LSTM layer. While a single LSTM can capture temporal dependencies at a lower level, stacking multiple LSTMs allows the model to capture more abstract and high-level patterns, leading to improved performance on complex sequence tasks such as machine translation and speech recognition.\n",
    "\n",
    "### 34. What are the key differences between RNN, LSTM, and GRU?\n",
    "- **RNN**: A basic Recurrent Neural Network (RNN) has a simple architecture where the output of a neuron is fed back into the network, allowing it to learn temporal dependencies. However, RNNs suffer from the vanishing gradient problem, making them difficult to train on long sequences.\n",
    "\n",
    "- **LSTM (Long Short-Term Memory)**: LSTM is an advanced RNN architecture designed to address the vanishing gradient problem. It uses gates (input, output, and forget gates) to control the flow of information, allowing it to capture long-range dependencies more effectively.\n",
    "\n",
    "- **GRU (Gated Recurrent Unit)**: GRU is similar to LSTM but with a simplified structure, using only two gates (update and reset gates). GRUs are computationally more efficient than LSTMs while still capturing long-term dependencies.\n",
    "\n",
    "### 35. Why is the attention mechanism important in sequence-to-sequence models?\n",
    "The attention mechanism allows a sequence-to-sequence model to focus on different parts of the input sequence when generating the output, instead of relying solely on the final hidden state. This is particularly useful for long sequences, where earlier inputs may be crucial for generating later outputs. Attention helps the model to \"attend\" to relevant information in the input sequence, improving performance in tasks like machine translation and text summarization by capturing more accurate dependencies between input and output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How do you perform word tokenization using NLTK and plot a word frequency distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHyCAYAAAAX5e80AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABc8UlEQVR4nO3deXhMZ/8G8Ptk32Wx1BJZBJXaYqmlbSSKUK/a2mq1UkppX4JY+v7SllLebopaWqqWoAullrZUtdbaBbHVEhFCJTQJwoQkk3l+f+jMOyOLJM4yk9yf68p1Zc6cOd/HyHLnOc8iCSEEiIiIiCoIO60bQERERCQnhhsiIiKqUBhuiIiIqEJhuCEiIqIKheGGiIiIKhSGGyIiIqpQGG6IiIioQmG4ISIiogrFQesGqM1gMODKlSvw9PSEJElaN4eIiIhKQQiBW7duoVatWrCzK7lvptKFmytXrsDf31/rZhAREVE5XLp0CXXq1CnxnEoXbjw9PQHce3O8vLxkvbZer8e+ffvQtm1bODgo/9aynu3XZD3brqdFTdZjPWuvqVS97Oxs+Pv7m36Pl6TShRvjrSgvLy9Fwo27uzu8vLxU+wJiPduuyXq2XU+LmqzHetZeU+l6pRlSwgHFREREVKEw3BAREVGFwnBDREREFQrDDREREVUoDDdERERUoTDcEBERUYXCcENEREQVCsMNERERVSgMN0RERFShaBpuPvzwQ7Ru3Rqenp6oXr06evXqhTNnzjzwdTt27EDLli3h4uKC4OBgzJ8/X4XWEhERkS3QNNzs2LEDw4cPx759+/Dbb79Br9ejS5cu0Ol0xb4mJSUFzzzzDJ566ikcOXIEb7/9NkaOHIkffvhBxZYTERGRtdJ0b6lNmzZZPF6yZAmqV6+OQ4cOITw8vMjXzJ8/H3Xr1sVnn30GAGjUqBESEhLw6aefom/fvko3mYiIiKycVY25uXnzJgDA19e32HP27t2LLl26WByLiopCQkIC8vPzFW1fSTYeT8Mb3xzBlL05OHb5pmbtICIiquysZldwIQTGjBmDJ598Eo0bNy72vPT0dNSoUcPiWI0aNaDX65GRkYGaNWtaPJebm4vc3FzT4+zsbAD3di3V6/Wytf9Cxm38fuoaAOBSlg5N61SR7drFMbZfzn9HZa6nRU3Ws+16WtRkPdaz9ppK1SvL9SQhhJC1ejkNHz4cGzZswK5du1CnTp1iz2vQoAEGDRqEuLg407Hdu3fjySefRFpaGh555BGL8ydNmoTJkycXus6GDRvg7u4uW/t3Xs7HouP3QlR0qDOeDnCU7dpERESVnU6nQ/fu3XHz5k14eXmVeK5V9NzExMTgxx9/xM6dO0sMNgDwyCOPID093eLYtWvX4ODgAD8/v0Lnx8XFYcyYMabH2dnZ8Pf3R9u2bR/45pRF3pm/sej4YQBAleq18MQTDWS7dnH0ej3279+PNm3awMFB+f/Kil5Pi5qsZ9v1tKjJeqxn7TWVqme881IamoYbIQRiYmKwdu1abN++HUFBQQ98Tbt27fDTTz9ZHNu8eTNatWoFR8fCvSXOzs5wdnYudNzBwUHWN72al6vp8+t39Kp90QLy/1sqez0tarKebdfToibrsZ6115S7XlmupemA4uHDh+Prr7/Gt99+C09PT6SnpyM9PR137twxnRMXF4fo6GjT4zfeeAMXL17EmDFjcOrUKSxevBiLFi3CuHHjtPgnmPi5O5k+z9JpN7CZiIiostM03MybNw83b95EREQEatasafpYuXKl6Zy0tDSkpqaaHgcFBWHjxo3Yvn07mjdvjilTpmD27NmaTwP38/hfuMnU5ZZwJhERESlJ89tSDxIfH1/oWIcOHXD48GEFWlR+bk4OcHW0x538AvbcEBERaciq1rmxdb7u98b8ZOryNG4JERFR5cVwIyPjuJsbOXkoMFjFDHsiIqJKh+FGRr7/hBuDuBdwiIiISH0MNzKynDHFcENERKQFhhsZ+bqbz5hiuCEiItICw42MLMLNbYYbIiIiLTDcyMjythTXuiEiItICw42MzHtuMthzQ0REpAmGGxmZr1LMAcVERETaYLiRka8bt2AgIiLSGsONjDigmIiISHsMNzJydbKHs/29zzkVnIiISBsMNzLzcpIAcMwNERGRVhhuZOb5T7i5zv2liIiINMFwIzNjz40Q9wIOERERqYvhRmbGnhuAg4qJiIi0wHAjMy9ns3DD6eBERESqY7iRGXtuiIiItMVwIzMvs3DDGVNERETqY7iRmWXPDW9LERERqY3hRmbmPTdcyI+IiEh9DDcy8+RtKSIiIk0x3MiMA4qJiIi0xXAjMyd7Ce7/bDDFqeBERETqY7hRgK/bvd3BOeaGiIhIfQw3CvDzuBdubuTkQ19g0Lg1RERElQvDjQJ83Z1Mn2dxfykiIiJVMdwowM883PDWFBERkaoYbhRg3nPDGVNERETqYrhRgHnPDQcVExERqYvhRgGWPTecDk5ERKQmhhsFcMwNERGRdhhuFGDec5PBMTdERESqYrhRgHGdGwDI4irFREREqmK4UYCPG2dLERERaYXhRgHODnbwdHEAwDE3REREamO4UYhxUHEGZ0sRERGpiuFGIX4ezgCA7Lt65Om5vxQREZFaGG4UYj5j6jr3lyIiIlINw41C/LgFAxERkSYYbhRiOR2c4YaIiEgtDDcK8XV3Nn2eybVuiIiIVMNwo5CqHrwtRUREpAWGG4VYbJ7JnhsiIiLVMNwoxM/sthTH3BAREamH4UYh5gOKuXkmERGRehhuFGK+vxR7boiIiNTDcKMQJwc7eP2zv1Qmt2AgIiJSDcONgqr+swVDJntuiIiIVMNwoyDjjKlbd/XI1Rdo3BoiIqLKgeFGQeaDiq/r8jVsCRERUeXBcKMg81WKMzjuhoiISBUMNwqqyv2liIiIVMdwoyCuUkxERKQ+hhsF+XmYbZ7JhfyIiIhUwXCjID+LnhuGGyIiIjVoGm527tyJHj16oFatWpAkCevWrXvga7755hs0a9YMbm5uqFmzJgYNGoTMzEzlG1sO5relsthzQ0REpApNw41Op0OzZs0wd+7cUp2/a9cuREdHY/DgwTh58iRWrVqFgwcPYsiQIQq3tHzMp4Kz54aIiEgdDloW79atG7p161bq8/ft24fAwECMHDkSABAUFIRhw4bhk08+UaqJD8V8fykOKCYiIlKHTY25ad++PS5fvoyNGzdCCIGrV69i9erV6N69u9ZNK5KjvR283RwBcCo4ERGRWjTtuSmr9u3b45tvvkG/fv1w9+5d6PV6PPvss5gzZ06xr8nNzUVu7v96TbKzswEAer0eer1e1vYZr2d+XV83J9zIyUfG7VxV6impotfToibr2XY9LWqyHutZe02l6pXlepIQQshavZwkScLatWvRq1evYs/5888/0alTJ8TGxiIqKgppaWkYP348WrdujUWLFhX5mkmTJmHy5MmFjm/YsAHu7u5yNb9Y/92Xg7PXDQCAr7q4w8leUrwmERFRRaPT6dC9e3fcvHkTXl5eJZ5rU+FmwIABuHv3LlatWmU6tmvXLjz11FO4cuUKatasWeg1RfXc+Pv7IzMz84FvTlnp9Xrs378fbdq0gYPDvU6x4d8ewa9/XgMA7BwXjlrerorWU1JFr6dFTdaz7Xpa1GQ91rP2mkrVy87Ohp+fX6nCjU3dlsrJySn0Rtnb2wMAistozs7OcHZ2LnTcwcFBsf9k82tX9XQxHc/ONaCuAjWV/LdUxnpa1GQ9266nRU3WYz1rryl3vbJcS9MBxbdv30ZiYiISExMBACkpKUhMTERqaioAIC4uDtHR0abze/TogTVr1mDevHk4f/48du/ejZEjR+Lxxx9HrVq1tPgnPJD5Qn7cPJOIiEh5mvbcJCQkIDIy0vR4zJgxAIBXX30V8fHxSEtLMwUdABg4cCBu3bqFuXPnYuzYsfD29kbHjh3x8ccfq9720jLfgoEzpoiIiJSnabiJiIgo9nYSAMTHxxc6FhMTg5iYGAVbJS+LzTO5SjEREZHibGqdG1vEVYqJiIjUxXCjMD93853BOeaGiIhIaQw3CjPvueGYGyIiIuUx3CjMx80J0j/r9mUw3BARESmO4UZh9naSaQPNLG6eSUREpDiGGxUYZ0xxthQREZHyGG5UYAw3OXkFuJNXoHFriIiIKjaGGxVUtZgOzltTRERESmK4UYH5Qn6cMUVERKQshhsVWKx1w3BDRESkKIYbFVisUsxBxURERIpiuFGBec8Np4MTEREpi+FGBdw8k4iISD0MNyqoys0ziYiIVMNwowLLnhveliIiIlISw40KvN2cYPfP/lKcCk5ERKQshhsVmO8vlcExN0RERIpiuFGJcTo4e26IiIiUxXCjEuO4mzv5BcjJ02vcGiIiooqL4UYlfh5mqxTz1hQREZFiGG5U4ufO6eBERERqYLhRCVcpJiIiUgfDjUp8zRby44wpIiIi5TDcqMT8thRnTBERESmH4UYlflylmIiISBUMNyrx4/5SREREqmC4UYnlgGKGGyIiIqUw3Kikiqsj7P/ZYIrr3BARESmH4UYldmb7S7HnhoiISDkMNyoyDirOuJ0LIYTGrSEiIqqYGG5UZBxUnKs3ICevQOPWEBERVUwMNyrytZgOzltTRERESmC4UVFV880zuQUDERGRIhhuVMSeGyIiIuUx3KjIfCE/zpgiIiJSBsONisy3YMjgbSkiIiJFMNyoyM9szE0Wb0sREREpguFGRRZjbnhbioiISBEMNyqq6m4+W4rhhoiISAkMNyrycnWAg2l/KY65ISIiUgLDjYokSYKPO/eXIiIiUhLDjcqMM6Yyb+dxfykiIiIFMNyozLjWTV6BAbdz9Rq3hoiIqOJhuFGZn9mgYt6aIiIikh/DjcrMp4NncK0bIiIi2THcqKwqt2AgIiJSFMONynzN17rhdHAiIiLZMdyozHzzTC7kR0REJD+GG5WZb56ZyTE3REREsmO4UZnF5pncGZyIiEh2DDcq4+aZREREymK4UZmXiwMc7Y37SzHcEBERyY3hRmWSJJl6bzJ5W4qIiEh2DDcaMK5SnKXj/lJERERyY7jRgHE6eH6BQPZd7i9FREQkJ4YbDZhPB+cqxURERPJiuNEAVykmIiJSjqbhZufOnejRowdq1aoFSZKwbt26B74mNzcX77zzDgICAuDs7Ix69eph8eLFyjdWRlylmIiISDkOWhbX6XRo1qwZBg0ahL59+5bqNS+88AKuXr2KRYsWISQkBNeuXYNeb1vjVrhKMRERkXI0DTfdunVDt27dSn3+pk2bsGPHDpw/fx6+vr4AgMDAQIVapxxfizE3vC1FREQkJ03DTVn9+OOPaNWqFT755BMsX74c7u7uePbZZzFlyhS4uroW+Zrc3Fzk5v4vQGRnZwMA9Hq97D0+xus96Lrerv972/++dbfc7ShtPblU9Hpa1GQ9266nRU3WYz1rr6lUvbJcTxJWstCKJElYu3YtevXqVew5Xbt2xfbt29GpUydMnDgRGRkZ+Pe//42OHTsWO+5m0qRJmDx5cqHjGzZsgLu7u1zNL5OrOgPe2pkDAGhb0wFvNnfRpB1ERES2QqfToXv37rh58ya8vLxKPNemwk2XLl3wxx9/ID09HVWqVAEArFmzBs899xx0Ol2RvTdF9dz4+/sjMzPzgW9OWen1euzfvx9t2rSBg0PxnWK37uoRNnULAOCJen5YOqiVovXkUtHraVGT9Wy7nhY1WY/1rL2mUvWys7Ph5+dXqnBjU7elatasidq1a5uCDQA0atQIQghcvnwZ9evXL/QaZ2dnODs7Fzru4OCg2H/yg67t7W4PJ3s75BUYkKnLe+h2KPlvqYz1tKjJerZdT4uarMd61l5T7npluZZNrXPzxBNP4MqVK7h9+7bp2NmzZ2FnZ4c6depo2LKykSTJNB2ci/gRERHJS9Nwc/v2bSQmJiIxMREAkJKSgsTERKSmpgIA4uLiEB0dbTq/f//+8PPzw6BBg/Dnn39i586dGD9+PF577bViBxRbK+OMKe4vRUREJC9Nw01CQgLCwsIQFhYGABgzZgzCwsIwceJEAEBaWpop6ACAh4cHfvvtN9y4cQOtWrXCyy+/jB49emD27NmatP9h+Hncu1WmNwhk37GtdXqIiIismaZjbiIiIkrstYiPjy907NFHH8Vvv/2mYKvUYb6QX4YuF1XcHDVsDRERUcVhU2NuKhJunklERKQMhhuN+JrvL8XNM4mIiGTDcKORquY7g7PnhoiISDYMNxrx5eaZREREimC40YifB8fcEBERKaFc4ebw4cM4fvy46fH69evRq1cvvP3228jL4y/q0vAzuy2VwTE3REREsilXuBk2bBjOnj0LADh//jxefPFFuLm5YdWqVXjrrbdkbWBF5cueGyIiIkWUK9ycPXsWzZs3BwCsWrUK4eHh+PbbbxEfH48ffvhBzvZVWO5O9nB2uPf2c8wNERGRfMoVboQQMBgMAIDff/8dzzzzDADA398fGRkZ8rWuApMkybTWDWdLERERyadc4aZVq1aYOnUqli9fjh07dqB79+4A7u0NVaNGDVkbWJEZt2C4npMHg4H7SxEREcmhXOFm5syZOHz4MEaMGIF33nkHISEhAIDVq1ejffv2sjawIjNOBy8wCNy8k69xa4iIiCqGcu0t1axZM4vZUkbTpk2Dg4Om21XZFPPp4Jm6PPiYrX1DRERE5VOunpvg4GBkZmYWOn737l00aNDgoRtVWfi5cwsGIiIiuZUr3Fy4cAEFBQWFjufm5uLy5csP3ajKwjjmBuB0cCIiIrmU6R7Sjz/+aPr8119/RZUqVUyPCwoKsGXLFgQFBcnXugrOfAuGDIYbIiIiWZQp3PTq1QvAvWnMr776qsVzjo6OCAwMxPTp02VrXEVX1XwhP651Q0REJIsyhRvj2jZBQUE4ePAgqlatqkijKgtfi53BOeaGiIhIDuWa2pSSkiJ3OyoliwHFvC1FREQki3LP296yZQu2bNmCa9eumXp0jBYvXvzQDasMLKaCc7YUERGRLMoVbiZPnoz3338frVq1Qs2aNSFJktztqhTcnBzg6miPO/kFnC1FREQkk3KFm/nz5yM+Ph4DBgyQuz2Vjq+7E/66cYebZxIREcmkXOvc5OXlcZsFmRhnTF3PyUMB95ciIiJ6aOUKN0OGDMG3334rd1sqJeNaNwYB3Mhh7w0REdHDKtdtqbt372LBggX4/fff0bRpUzg6Olo8P2PGDFkaVxncv0qx+WMiIiIqu3KFm2PHjqF58+YAgBMnTlg8x8HFZWM+HTzjdh7q19CwMURERBVAucLNtm3b5G5HpWW+BQNnTBERET28co25IfmY34biKsVEREQPr1w9N5GRkSXeftq6dWu5G1TZWKxSzOngRERED61c4cY43sYoPz8fiYmJOHHiRKENNalkFqsUs+eGiIjooZUr3MycObPI45MmTcLt27cfqkGVDcfcEBERyUvWMTevvPIK95UqIz/zncF5W4qIiOihyRpu9u7dCxcXFzkvWeG5OtnDzckeAHcGJyIikkO5bkv16dPH4rEQAmlpaUhISMCECRNkaVhl4ufhhJysO7wtRUREJINyhZsqVapYPLazs0PDhg3x/vvvo0uXLrI0rDLxdXfGpaw7pv2l7O24ECIREVF5lSvcLFmyRO52VGpV/xlULMS9DTSrcgsGIiKicitXuDE6dOgQTp06BUmSEBoairCwMLnaVan43rfWDcMNERFR+ZUr3Fy7dg0vvvgitm/fDm9vbwghcPPmTURGRmLFihWoVq2a3O2s0AqvUuypXWOIiIhsXLlmS8XExCA7OxsnT55EVlYWrl+/jhMnTiA7OxsjR46Uu40VHlcpJiIikk+5em42bdqE33//HY0aNTIdCw0Nxeeff84BxeVgvkoxZ0wRERE9nHL13BgMBjg6OhY67ujoCIPB8NCNqmwsx9xwCwYiIqKHUa5w07FjR4waNQpXrlwxHfvrr78QGxuLp59+WrbGVRZVLcbcsOeGiIjoYZQr3MydOxe3bt1CYGAg6tWrh5CQEAQFBeHWrVuYM2eO3G2s8O6fLUVERETlV64xN/7+/jh8+DB+++03nD59GkIIhIaGolOnTnK3r1Lg5plERETyKVPPzdatWxEaGors7GwAQOfOnRETE4ORI0eidevWeOyxx/DHH38o0tCKzMXRHh7O93Jmho5jboiIiB5GmcLNZ599htdffx1eXl6FnqtSpQqGDRuGGTNmyNa4ysTYe8OeGyIioodTpnBz9OhRdO3atdjnu3TpgkOHDj10oyoj43TwGzn5yC/gjDMiIqLyKlO4uXr1apFTwI0cHBzw999/P3SjKiPzhfyu57D3hoiIqLzKFG5q166N48ePF/v8sWPHULNmzYduVGXk5/6/6eC8NUVERFR+ZQo3zzzzDCZOnIi7d+8Weu7OnTt477338K9//Uu2xlUmvh6cDk5ERCSHMk0Ff/fdd7FmzRo0aNAAI0aMQMOGDSFJEk6dOoXPP/8cBQUFeOedd5Rqa4Vmsb8Ue26IiIjKrUzhpkaNGtizZw/efPNNxMXFQQgBAJAkCVFRUfjiiy9Qo0YNRRpa0fl5cAsGIiIiOZR5Eb+AgABs3LgR169fx7lz5yCEQP369eHj46NE+yoNjrkhIiKSR7lWKAYAHx8ftG7dWs62VGrmqxRncMwNERFRuZVrbymSn/nmmVlcpZiIiKjcGG6shI/7/9YP4mwpIiKi8mO4sRLODvbwdLl3l5BjboiIiMpP03Czc+dO9OjRA7Vq1YIkSVi3bl2pX7t79244ODigefPmirVPbcbp4BmcLUVERFRumoYbnU6HZs2aYe7cuWV63c2bNxEdHY2nn35aoZZpw++fcTfZd/XI03N/KSIiovIo92wpOXTr1g3dunUr8+uGDRuG/v37w97evky9PdbO9779pWp4uWjYGiIiItukabgpjyVLliA5ORlff/01pk6d+sDzc3NzkZv7v9s82dnZAAC9Xg+9Xi9r24zXK+91fVz/999x7WYO/NxK/u952HplVdHraVGT9Wy7nhY1WY/1rL2mUvXKcj1JGJcZ1pgkSVi7di169epV7DlJSUl48skn8ccff6BBgwaYNGkS1q1bh8TExGJfM2nSJEyePLnQ8Q0bNsDd3V2Glstn9dlc/JScDwAY39oFjavaXPYkIiJShE6nQ/fu3XHz5k14eXmVeK7N/PYsKChA//79MXnyZDRo0KDUr4uLi8OYMWNMj7Ozs+Hv74+2bds+8M0pK71ej/3796NNmzZwcCj7W3tWuoCfks8AAGoG1scTzWopWq+sKno9LWqynm3X06Im67GetddUqp7xzktp2Ey4uXXrFhISEnDkyBGMGDECAGAwGCCEgIODAzZv3oyOHTsWep2zszOcnZ0LHXdwcFDsP7m8167u5Wr6/PqdglJfQ8l/S2Wsp0VN1rPtelrUZD3Ws/aactcry7VsJtx4eXnh+PHjFse++OILbN26FatXr0ZQUJBGLZOP+YBirlJMRERUPpqGm9u3b+PcuXOmxykpKUhMTISvry/q1q2LuLg4/PXXX1i2bBns7OzQuHFji9dXr14dLi4uhY7bKm6eSURE9PA0DTcJCQmIjIw0PTaOjXn11VcRHx+PtLQ0pKamatU81fl5cPNMIiKih6VpuImIiEBJk7Xi4+NLfP2kSZMwadIkeRulIR8389tSDDdERETlwb2lrIiTgx28/tlfKpNbMBAREZULw42VqfrPFgyZ7LkhIiIqF4YbK2OcMXXrrh65+gKNW0NERGR7GG6sjPmg4uu6fA1bQkREZJsYbqyMr9l08AyOuyEiIiozhhsrU9WDM6aIiIgeBsONlTFfpTiTqxQTERGVGcONlfHz+N9tqUwu5EdERFRmDDdWxs+i54bhhoiIqKwYbqyMxeaZ7LkhIiIqM4YbK2M+FZxjboiIiMqO4cbKmO8vxdtSREREZcdwY2Uc7e3g7eYIgAOKiYiIyoPhxgoZx91wnRsiIqKyY7ixQlX/WaX4dq4ed/O5vxQREVFZMNxYIYsZU+y9ISIiKhOGGyvkxy0YiIiIyo3hxgqZL+THzTOJiIjKhuHGCplvwcCeGyIiorJhuLFCFptncjo4ERFRmTDcWCHLVYoZboiIiMqC4cYK+bmb7wzOMTdERERlwXBjhThbioiIqPwYbqyQj5sTJOne5xkMN0RERGXCcGOF7O0k0waaWdwZnIiIqEwYbqyUccYUZ0sRERGVDcONlTKGm5y8AtzJ4/5SREREpcVwY6WqWkwH560pIiKi0mK4sVLcPJOIiKh8GG6slOVaNww3REREpcVwY6W4SjEREVH5MNxYKa5STEREVD4MN1aKY26IiIjKh+HGSlXlbSkiIqJyYbixUuY9N7wtRUREVHoMN1bK280Jdv/sL8XbUkRERKXHcGOlzPeXyuBUcCIiolJjuLFixung7LkhIiIqPYYbK2Ycd3MnvwA5eXqNW0NERGQbGG6smJ8HVykmIiIqK4YbK+bnzungREREZcVwY8XMVynO4s7gREREpcJwY8V8zRby44wpIiKi0mG4sWJ+3IKBiIiozBhurJgfVykmIiIqM4YbK+bH/aWIiIjKjOHGipkPKOZUcCIiotJhuLFiVVwdYf/PBlMcc0NERFQ6DDdWzM5sfymOuSEiIiodhhsrZxxUnKnLgxBC49YQERFZP4YbK2ccVJyrN0CXV6Bxa4iIiKwfw42V8zVf64aDiomIiB6I4cbKVTXfPJNbMBARET0Qw42V87VYyI89N0RERA/CcGPlzBfy43RwIiKiB2O4sXLmWzBk8LYUERHRA2kabnbu3IkePXqgVq1akCQJ69atK/H8NWvWoHPnzqhWrRq8vLzQrl07/Prrr+o0ViN+ZmNuOKCYiIjowTQNNzqdDs2aNcPcuXNLdf7OnTvRuXNnbNy4EYcOHUJkZCR69OiBI0eOKNxS7ViMueFtKSIiogdy0LJ4t27d0K1bt1Kf/9lnn1k8/uCDD7B+/Xr89NNPCAsLk7l11qGq+f5SDDdEREQPpGm4eVgGgwG3bt2Cr69vsefk5uYiN/d/Y1Wys7MBAHq9Hnq9Xtb2GK8n53XdHAEHOwl6g0DGrbsW11aiXkkqej0tarKebdfToibrsZ6111SqXlmuJwkrWdNfkiSsXbsWvXr1KvVrpk2bho8++ginTp1C9erVizxn0qRJmDx5cqHjGzZsgLu7e3mbq6qRW3W4mSvg6yJhZqRttJmIiEhOOp0O3bt3x82bN+Hl5VXiuTbbc/Pdd99h0qRJWL9+fbHBBgDi4uIwZswY0+Ps7Gz4+/ujbdu2D3xzykqv12P//v1o06YNHBzke2trHtmNm+m3cSsfaN++PSRJUrRecSp6PS1qsp5t19OiJuuxnrXXVKqe8c5LadhkuFm5ciUGDx6MVatWoVOnTiWe6+zsDGdn50LHHRwcFPtPlvva92ZM3UZ+gcDdAsDTxfLaSv5bilLR62lRk/Vsu54WNVmP9ay9ptz1ynItm1vn5rvvvsPAgQPx7bffonv37lo3RxV+5oOKOR2ciIioRJr23Ny+fRvnzp0zPU5JSUFiYiJ8fX1Rt25dxMXF4a+//sKyZcsA3As20dHRmDVrFtq2bYv09HQAgKurK6pUqaLJv0EN908HD6zKcTdERETF0bTnJiEhAWFhYaZp3GPGjEFYWBgmTpwIAEhLS0Nqaqrp/C+//BJ6vR7Dhw9HzZo1TR+jRo3SpP1qqephvr8UVykmIiIqiaY9NxEREShpslZ8fLzF4+3btyvbICvla3ZbivtLERERlczmxtxURuabZ3IhPyIiopIx3NgA880zOaCYiIioZAw3NsBi80zuDE5ERFQihhsbwM0ziYiISo/hxgZ4uTjA0f7eqsS8LUVERFQyhhsbIEmSqfcmk7eliIiISsRwYyOMqxRn6fJKnD5PRERU2THc2AjjdPD8AoHsu+psW09ERGSLGG5shPl0cC7kR0REVDyGGxvha7F5JsfdEBERFYfhxkZwlWIiIqLSYbixEVylmIiIqHQYbmyEr8WYG96WIiIiKg7DjY0w34Ihgz03RERExWK4sRGcLUVERFQ6DDc2wnJAMW9LERERFYfhxkZ4ODvAyf7efxcHFBMRERWP4cZGSJJk6r3hVHAiIqLiMdzYEOOMqeu6PBgM3F+KiIioKAw3NsQ4Y0pvEMi+m69xa4iIiKwTw40NsVjIj7emiIiIisRwY0M4HZyIiOjBGG5siK/5dHBunklERFQkhhsbUtV8Z3D23BARERWJ4caG+HLzTCIiogdiuLEh5qsUc8wNERFR0RhubIifu/nmmRxzQ0REVBSGGxviy54bIiKiB2K4sSHuTvZwduD+UkRERCVhuLEhkiSZ1rrhbCkiIqKiMdzYGOMWDNdzuL8UERFRURhubIxxOniBQeAm95ciIiIqhOHGxvh5cK0bIiKikjDc2BjuL0VERFQyhhsbYxxzA3BQMRERUVEYbmyML3tuiIiISsRwY2Oqmo+5YbghIiIqhOHGxviabcHAnhsiIqLCGG5sjOWAYk4FJyIiuh/DjY2xmAqu4+aZRERE92O4sTFuTg5wdbQHwJ4bIiKiojDc2CBf7i9FRERULIYbG2ScMXUjJw8Gwf2liIiIzDHc2CBjz41BANyBgYiIyBLDjQ0ynw5+K489N0REROYYbmyQ+UJ+2XkGDVtCRERkfRhubJD5Fgy3eFuKiIjIAsONDTLfPJM9N0RERJYYbmyQn0XPDcfcEBERmWO4sUF+FmNuGG6IiIjMMdzYIF/23BARERWL4cYG+ZlNBc/OZbghIiIyx3Bjg1yd7OHmdG9/KfbcEBERWWK4sVHGcTccc0NERGSJ4cZGGVcp1uUDBQYGHCIiIiOGGxtV9Z9BxQL3NtAkIiKiezQNNzt37kSPHj1Qq1YtSJKEdevWPfA1O3bsQMuWLeHi4oLg4GDMnz9f+YZaIfMZU5k6hhsiIiIjTcONTqdDs2bNMHfu3FKdn5KSgmeeeQZPPfUUjhw5grfffhsjR47EDz/8oHBLrY/5KsUMN0RERP/joGXxbt26oVu3bqU+f/78+ahbty4+++wzAECjRo2QkJCATz/9FH379lWoldbJfJXiLIYbIiIiE03DTVnt3bsXXbp0sTgWFRWFRYsWIT8/H46Ojhq1TH3mqxTP3XYea46kKV5TCIHr1+9g0blDkCSpwtXToibr2XY9LWqyHutZe01jPb96t9C4jo/i9YpiU+EmPT0dNWrUsDhWo0YN6PV6ZGRkoGbNmoVek5ubi9zcXNPj7OxsAIBer4der5e1fcbryX3dovi6/e+/LunabSRdu614TZOMDPVqaVFPi5qsZ9v1tKjJeqxn5TWzbt+V9fdhWa5lU+EGQKHUKYQo8rjRhx9+iMmTJxc6vm/fPri7u8vfQAD79+9X5LrmCgwCQVXskHKTu4ITEZH1OfXnKYirZ2W7nk6nK/W5NhVuHnnkEaSnp1scu3btGhwcHODn51fka+Li4jBmzBjT4+zsbPj7+6Nt27bw8vKStX16vR779+9HmzZt4OCg/Fvbvm0+tu3ej1YtW6pST6/XI+HQoQpbT4uarGfb9bSoyXqsZ+01jfXC2z0OF2enB7+glIx3XkrDpsJNu3bt8NNPP1kc27x5M1q1alXseBtnZ2c4OzsXOu7g4KDYf7KS176fu6MEX09X1b5gK3I9LWqynm3X06Im67Getdc01nNxdpK1XlmupelU8Nu3byMxMRGJiYkA7k31TkxMRGpqKoB7vS7R0dGm89944w1cvHgRY8aMwalTp7B48WIsWrQI48aN06L5REREZIU07blJSEhAZGSk6bHx9tGrr76K+Ph4pKWlmYIOAAQFBWHjxo2IjY3F559/jlq1amH27NmVbho4ERERFU/TcBMREWEaEFyU+Pj4Qsc6dOiAw4cPK9gqIiIismXcW4qIiIgqFIYbIiIiqlAYboiIiKhCYbghIiKiCoXhhoiIiCoUhhsiIiKqUBhuiIiIqEJhuCEiIqIKheGGiIiIKhSb2jhTDsYVkcuyu2hp6fV66HQ6ZGdnq7Y5GevZdk3Ws+16WtRkPdaz9ppK1TP+3i5pZwOjShdubt26BQDw9/fXuCVERERUVrdu3UKVKlVKPEcSpYlAFYjBYMCVK1fg6ekJSZJkvXZ2djb8/f1x6dIleHl5yXpt1lO+nhY1Wc+262lRk/VYz9prKlVPCIFbt26hVq1asLMreVRNpeu5sbOzQ506dRSt4eXlpdoXLetVjJqsZ9v1tKjJeqxn7TWVqPegHhsjDigmIiKiCoXhhoiIiCoUhhsZOTs747333oOzszPr2WA9LWqynm3X06Im67GetdfU4t94v0o3oJiIiIgqNvbcEBERUYXCcENEREQVCsMNERERVSgMN0RERFShMNwQERFRhVLpVigmIrIlqamp8Pf3L7RdjBACly5dQt26dTVqme37888/kZqairy8PIvjzz77rEYtkkdKSgqCgoK0boamOBWcSi0vLw8pKSmoV6+earvZaiE7Oxtbt25Fw4YN0ahRI9mvf+PGDRw4cADXrl2DwWCweC46Olr2etbwy1Hp93T58uWYP38+UlJSsHfvXgQEBOCzzz5DUFAQevbsKWutS5cuQZIk0zYuBw4cwLfffovQ0FAMHTpU1loAYG9vj7S0NFSvXt3ieGZmJqpXr46CggLZa6pt2bJl6NevX6F1UfLy8rBixQrZvy/Onz+P3r174/jx45AkybTLtPF7xNbfU3t7e4SHh2Pw4MF47rnn4OLiolrtvLy8In+2qR3CGW4U0qlTJ5w/fx7nz5+X5XrHjh0r9blNmzaVpaZRTk4OYmJisHTpUgDA2bNnERwcjJEjR6JWrVr4v//7P1nrzZ49u8jjkiTBxcUFISEhCA8Ph729vSz1XnjhBYSHh2PEiBG4c+cOmjVrhgsXLkAIgRUrVqBv376y1AGAn376CS+//DJ0Ol2hzVslSUJWVpZstYy0+OWo5ns6b948TJw4EaNHj8Z///tfnDhxAsHBwYiPj8fSpUuxbds22WoBwFNPPYWhQ4diwIABSE9PR8OGDfHYY4/h7NmzGDlyJCZOnChrPTs7O1y9ehXVqlWzOH7x4kWEhoZCp9PJUsfHx6fUmwnL/XWq9tdojx49YG9vj6+++grBwcE4cOAAMjMzMXbsWHz66ad46qmnHrqGlu/niRMnsHjxYnzzzTfIzc1Fv379MHjwYDz++OOy1jGXlJSE1157DXv27LE4LoSAJEnqB0ZBipg7d66YNGmSbNeTJEnY2dkJSZKK/DA+Z2dnJ1tNo5EjR4qWLVuKP/74Q7i7u4vk5GQhhBDr168XzZs3l71eYGCgcHd3F5IkCV9fX+Hj4yMkSRLu7u6iRo0aQpIkUa9ePZGamipLvRo1aojExEQhhBDffPONCAkJETqdTnzxxRey//vq168vRo0aJXQ6nazXLYkkSeLatWuFjl+4cEG4ubkpUlPN97RRo0Zi7dq1QgghPDw8TF+fx48fF35+frLWEkIIb29vcfr0aSGEELNmzRLt27cXQgjx66+/iqCgINnqxMbGitjYWGFnZyeGDRtmehwbGytGjhwp2rRpY6oth/j4eNPH9OnThY+Pj3jxxRfFrFmzxKxZs8SLL74ofHx8xIwZM2SraVTc12hiYqLw8fGRvZ6fn584evSoEEIILy8v0//nli1bZPv61PL9NMrPzxdr1qwRzz77rHB0dBShoaFi+vTpRb7XD6t9+/YiPDxcbNy4URw5ckQkJiZafKiN4cZGXLhwodQfcqtbt67Yu3evEMLyl0dSUpLw9PSUvd63334rIiIixLlz50zHkpKSRMeOHcWKFSvEpUuXxBNPPCH69u0rSz0XFxdTUBowYID4z3/+I4QQ4uLFi8Ld3V2WGkZubm6m909pav9yNKfme+ri4mL6ujf/+jx79qxwcXGRtZYQQri7u4uUlBQhhBA9evQQH330kRDi3r9NznoREREiIiJCSJIk2rdvb3ocEREhunTpIoYOHSrOnj0rWz1zffr0EXPmzCl0fM6cOaJnz56y1WnevLkICwsTdnZ2okmTJiIsLMz00bRpU+Hp6Smef/552eoZeXt7m75OgoODxdatW4UQQpw7d064urrKXk+t97M4d+/eFTNmzBDOzs5CkiTh5OQkBgwYIK5cuSJbDTc3N3Hq1CnZrvewKu7AiQomICBAs9p///13oe5iANDpdKXudi2Ld999Fz/88APq1atnOhYSEoJPP/0Uffv2xfnz5/HJJ5/IdmvD398fe/fuha+vLzZt2oQVK1YAAK5fvy77veqoqCgkJCQgODhY1usW5ciRIwDudQsfP34cTk5OpuecnJzQrFkzjBs3TpHaar6nQUFBSExMLPQ98ssvvyA0NFTWWgDw2GOPYf78+ejevTt+++03TJkyBQBw5coV+Pn5yVbHeDtt0KBBmDVrFry8vGS79oP8+uuv+Pjjjwsdj4qKkvU2dK9evQAAiYmJiIqKgoeHh+k5JycnBAYGynoL06hx48Y4duwYgoOD0aZNG3zyySdwcnLCggULFPneVOv9vF9CQgIWL16MFStWwN3dHePGjcPgwYNx5coVTJw4ET179sSBAwdkqRUaGoqMjAxZriULrdMVld/JkyfFL7/8ItavX2/xIbfw8HAxe/ZsIcS9v4zPnz8vhBBi+PDhIioqSvZ6rq6u4uDBg4WOHzhwwPRXVUpKimw9AJ9//rlwcHAQ3t7eomnTpqKgoEAIIcTs2bNFRESELDWMFi5cKOrWrSvee+89sXr1asX/74QQYuDAgeLmzZuKXLs45u9ps2bNFH1PFy9eLGrXri1WrFgh3N3dxXfffSemTp1q+lxu27ZtE97e3sLOzk4MGjTIdDwuLk707t1b9nrp6enFPme8tSK3unXrik8++aTQ8U8++UTUrVtX9nrx8fHizp07sl+3OJs2bRI//PCDEEKI5ORk0ahRIyFJkqhatarYsmWL7PXUfj+nT58uGjduLBwdHUXPnj3FTz/9ZPoeNEpKShL29vay1dyyZYto166d2LZtm8jIyBA3b960+FAbw40NSk5OFk2bNi00DsfOzk6RMTe7d+8Wnp6e4o033hAuLi5i1KhRolOnTsLd3V0kJCTIXu+ZZ54RLVq0EIcPHzYdO3z4sGjZsqXo3r27EEKIH3/8UTRu3Fi2mgkJCWLNmjXi9u3bpmM///yz2L17t2w1hBDFjplSaryUuaSkJLFp0yaRk5MjhBDCYDAoWs/4nt66dct07Oeffxa7du2SvdaCBQtE3bp1Te9lnTp1xMKFC2WvY6TX60VWVpbFsZSUFHH16lXZa1WrVq3I4Dtt2jRFbrsJIcSSJUuEnZ2deOaZZ8SUKVPElClTRPfu3YW9vb1YsmSJIjWFuPc1s3z5cvH1119bfP+rITMzU7HvCbXfz5CQEPHBBx+ItLS0Ys/Jzc0V8fHxstW8//eQ8UONn21FtkcIzpayNWqM9L/f8ePH8emnn+LQoUMwGAxo0aIF/vOf/6BJkyay10pPT8eAAQOwZcsWODo6AgD0ej2efvppLF++HDVq1MC2bduQn5+PLl26lKvGmDFjMGXKFLi7u2PMmDElnjtjxoxy1bAWWVlZeP7557Ft2zZIkoSkpCQEBwdj8ODB8Pb2xvTp02Wr9f7771s8lnvm0INkZGTAYDAUeRvVVk2fPh3vvvsuXn31VcycORNZWVkYMGAATp48ia+++kqxNVn279+P2bNn49SpUxBCIDQ0FCNHjkSbNm1kr3Xt2jW8+OKL2L59O7y9vSGEwM2bNxEZGYkVK1YUmilmi9R8P7WwY8eOEp/v0KGDSi25h+HGBlWtWhVbt25F06ZNUaVKFRw4cAANGzbE1q1bMXbsWNNYC1t3+vRpnD17FkIIPProo2jYsKFs146MjMTatWvh7e2NyMjIYs+TJAlbt26Vra4WoqOjce3aNSxcuBCNGjXC0aNHERwcjM2bNyM2NhYnT56UrdagQYNMn0uShMWLF8t27eK8//77ePLJJ9GxY0eL4zqdDtOnT5clYLVo0QJbtmyBj48PwsLCShxrdvjw4Yeud7+jR4/ilVdewd27d5GVlYW2bdti8eLFqFGjhuy1tNCvXz8kJydj+fLlpnWQ/vzzT7z66qsICQnBd999p3ELyy8/Px9Dhw7FhAkTVBlrBwBLliyBh4cHnn/+eYvjq1atQk5ODl599VVV2qElhhsb5OPjg0OHDiE4OBj16tXDwoULERkZieTkZDRp0gQ5OTmy1svOzi7yuCRJcHZ2thioSiW7v2fjfkr0dDzyyCP49ddf0axZM3h6eprCTUpKCpo0aYLbt2/LXlNNdnZ2cHR0xIcffmjRC3f16lXUqlVLlvU1Jk+ejPHjx8PNzQ2TJ08u8dz33nvvoevd79atW3j99dfxww8/AAAWLlyo+C8og8GAc+fOFbkgW3h4uKy1qlSpgt9//x2tW7e2OH7gwAF06dIFN27ckLWe2ry9vXH48GHVwk3Dhg0xf/78Qn+47dixA0OHDsWZM2cUqXvjxg0sWrQIp06dgiRJCA0NxWuvvYYqVaooUq8knC1lg9Qe6e/t7V3iX6p16tTBwIED8d5778HO7uG3KysoKEB8fDy2bNlS5A9WW+5JWbt2rcXj/Px8pKSkwMHBAfXq1VMk3Oh0Ori5uRU6npGRUWhFWFu1bNkyjBgxAseOHcOCBQtkD9zmgUWJ8FKS3bt345VXXoGfnx+OHTuG3bt3IyYmBhs2bMCXX34JHx8f2Wvu27cP/fv3x8WLF3H/379KLMhmMBhMt6DNOTo6Fvr+t0W9e/fGunXrHngLXC4XL14scvuFgIAApKamKlIzISEBUVFRcHV1xeOPPw4hBGbMmIH//ve/2Lx5M1q0aKFI3eIw3Nigd99917Qq6dSpU/Gvf/0LTz31FPz8/LBy5UrZ68XHx+Odd97BwIEDTV+0Bw8exNKlS/Huu+/i77//xqeffgpnZ2e8/fbbD11v1KhRiI+PR/fu3dG4cWNFpptrpahbhtnZ2Rg4cCB69+6tSM3w8HAsW7bMNGVZkiQYDAZMmzYNERERitRUW2RkJPbt24cePXogIiKiUIhUglrLzHfs2BGxsbGYMmUKHB0d0ahRI0RGRmLAgAFo0qQJLl++LGs9AHjjjTfQqlUrbNiwATVr1lT8e7Bjx44YNWoUvvvuO9SqVQsA8NdffyE2NhZPP/20orXVEBISgilTpmDPnj1o2bIl3N3dLZ4fOXKkrPWqV6+OY8eOITAw0OL40aNHZV2uwFxsbCyeffZZfPXVV6btefR6PYYMGYLRo0dj586ditQtlupDmEkRSo7079ixo1i5cmWh4ytXrhQdO3YUQgixbNky0bBhQ1nq+fn5iQ0bNshyLVtx/PhxERAQoMi1T548KapVqya6du0qnJycxHPPPScaNWokatSoYbFQoq2ys7MzzVK6efOmiIqKEnXq1BE///yzIrM0zpw5I5588knVZoVs3769yOMFBQXi/fffl72eEPcWZEtKSlLk2kVJTU0VYWFhwtHRUQQHB4t69eoJR0dH0aJFC3Hp0iXV2qGUwMDAYj/kXNXaaPz48SIgIEBs3bpV6PV6odfrxZYtW0RAQIAYO3as7PWEuLeYZlGL+J08eVKRhREfhD03Nkav18PFxQWJiYlo3Lix6bivr69iNffu3Yv58+cXOh4WFoa9e/cCAJ588knZujudnJwQEhIiy7VsxY0bN3Dz5k1Frh0aGopjx45h3rx5sLe3h06nQ58+fTBs2DC89957qgz6VZIwu23i5eWFjRs3YvTo0aYF4uQ2aNAgODg44Oeff1alV8M4y+TcuXNITk5GeHg4XF1dIUkSJkyYoEjNNm3a4Ny5c6p9H/r7++Pw4cP47bffcPr0adNsok6dOqlSX2kpKSmq1ps6dSouXryIp59+2tSLYjAYEB0djQ8++ECRml5eXkhNTcWjjz5qcfzSpUvw9PRUpGZJGG5sjIODAwICAlTdhKxOnTpYtGgRPvroI4vjixYtgr+/P4B7G9zJde9/7NixmDVrFubOnVuhbkkBhTcFFUIgLS0Ny5cvR9euXRWr+8gjjxQaCHv06FEsXbrU5sPNkiVLLAYs2tnZYfbs2QgLC1OkKzwxMRGHDh0q9ENcKZmZmXjhhRcKTeUfMmQIfHx88Omnn8peMyYmBmPHjkV6ejqaNGlSaDyM3JvzGnXu3BmdO3dW5NrWQty3A7kSnJycsHLlSkyZMgVHjx6Fq6srmjRpouhK98bNOT/99FO0b98ekiRh165dGD9+PF566SXF6haHs6Vs0JIlS7Bq1Sp8/fXXivbYGP344494/vnn8eijj6J169aQJAkHDx7E6dOnsXr1avzrX//CvHnzkJSUJMuaML1798a2bdvg6+uLxx57rNAP1jVr1jx0DTUdO3YMjRs3hp2dXaFBfnZ2dqhWrRo6duyIuLg4Vf/COXr0KFq0aKH+br02rnXr1pg5cyaefPJJVeqpOZXfqKSJAXINKL4/6JdE7jEpWli2bBmmTZuGpKQkAECDBg0wfvx4DBgwQOOWySMvLw/jx4/H/PnzodfrAdwbEP7mm2/io48+Un3yAsONDQoLC8O5c+eQn5+PgICAQoPTlFhn4+LFi5g/fz7OnDljWndm2LBhhQasycF8rZSiLFmyRPaaSrK3t0daWhqqV6+OoKAgHDx4EFWrVtW6WTYdbmbPno2hQ4fCxcWlxF+SkiQhJibmoeuZL4eQkJCAd999Fx988EGRvRpy7wGlxVT+ixcvlvi8HD0ARc3mKYokSTh//vxD19PSjBkzMGHCBIwYMQJPPPEEhBDYvXs3Pv/8c0ydOhWxsbGy1tNyxmlOTg6Sk5MhhEBISEiRMzXVwHBjg7RYZ4PKz8/PDxs3bkSbNm1gb2+P9PR0q1hx1ZbDTVBQEBISEuDn51fiL0m5fjHa2dlZ3EYQQhS6rWA8Jvf76enpicOHD6N+/foW4ebgwYPo2rUrMjMzZa1n7s8//0Rqairy8vJMxyRJQo8ePRSrqcZtG7UFBQVh8uTJiI6Otji+dOlSTJo0SfYxOSNGjDDNOC1qXNjMmTNlrWeNOObGBmkVXnJycgr9oAOUu/9eUfTt2xcdOnRAzZo1AQCtWrWCvb19kefK+Rdqnz59SnzelhdGM/9loMZgTeMO3QBw4cIF+Pv7F/o/NBgMiqwhUtJU/pJW134Y58+fR+/evXH8+HFIklQocCgRiBctWoSZM2eabtvUr18fo0ePxpAhQ2Svpba0tDS0b9++0PH27dsjLS1N9norVqzA999/j2eeeUb2a5vr06cP4uPj4eXl9cCfN2oPJ2C4oQf6+++/MWjQIPzyyy9FPi/HDzqtl7dX0oIFC9CnTx+cO3cOI0eOxOuvv67K2JoHrQpapUqVQn9JUtHM98Xp2LGj6TajuczMTHTq1En2lYON6xElJCQgLy8Pb731Fk6ePImsrCzs3r1b1lpGo0aNQlBQEH7//XcEBwdj//79yMrKMu1fJ7cJEyZg5syZiImJQbt27QDcm6UZGxuLCxcuYOrUqbLXVFNISAi+//77QuuArVy5EvXr15e9nlozTqtUqWL6We3l5WVVvW0MNzbo/i7y+8n9V9Xo0aNx/fp17Nu3z7Qn09WrVzF16lTZNl3s2bOnacCZUlN4tWScCXXo0CGMGjVKlXBja2OTykvt8QVF3ZICgNu3b8PFxUXWWgDg4eGBxMREfPnllxZT+YcPH478/HzZ6wH3gsXWrVtRrVo12NnZwd7eHk8++SQ+/PBDjBw5Uvb96+bNm4evvvrKYlbNs88+i6ZNmyImJsbmw83kyZPRr18/7Ny5E0888YRpJtGWLVvw/fffy15PrRmn5j9j4uPjFatTHgw3NqioJfyPHDmCpUuXPnA8Tnls3boV69evR+vWrWFnZ4eAgAB07twZXl5e+PDDD9G9e/eHrqHl8vZqqiyBQ01qrWhtXDrfuL6M+UDJgoIC7N+/H82bN5e9blBQENLS0gp9b2dmZqJOnTqK3CIqKCiAh4cHgHsb9V65cgUNGzZEQECAIvsSFRQUoFWrVoWOt2zZ0jTzxpb17dsXBw4cwIwZM7Bu3TrTOj4HDhxAWFiY7PV27dqFbdu24ZdfflFtxmnHjh2xZs0aeHt7WxzPzs5Gr169VN82h+HGBvXs2bPQseeeew6PPfYYVq5cicGDB8taT6fTmbrgfX198ffff6NBgwZo0qSJIreILl26BEmSUKdOHQD3Ns/79ttvERoaiqFDh8pej2ybWuMLjL0VQggcP37cYv8qJycnNGvWDOPGjZO9bnFzPpTqKQLU37/ulVdewbx58wotJbFgwQK8/PLLstdT28svv4yIiAhMnDgRDRo0ULyet7e3Ytu5FGf79u2FxmMCwN27d/HHH3+o2haA4aZCadOmDV5//XXZr9uwYUOcOXMGgYGBaN68Ob788ksEBgZi/vz5pkGycurfvz+GDh2KAQMGID09HZ06dULjxo3x9ddfIz09XZHNJcl2qTW+wDioeNCgQZg1a5bsU77vZ95TNHHiRNV6igD1968D7g0o3rx5M9q2bQvg3uadly5dQnR0tMWGk3KspaU2Dw8PTJ8+HW+88QZq1KiBDh06oEOHDoiIiFBkMUg1e4iPHTtm+vzPP/9Eenq66XFBQQE2bdqE2rVrq9YeI04FryDu3LmDuLg4/PLLL7J3G3/zzTfIz8/HwIEDceTIEURFRSEzMxNOTk6Ij49Hv379ZK3n4+ODffv2oWHDhpg9ezZWrlyJ3bt3Y/PmzXjjjTdsfs0Lktf06dNx/vz5CreitXEm1I4dO9CuXbtCPUWBgYEYN26cIgNSi5KVlQUfHx9F3uPSzvqSJEn12xtySk9Px/bt27F9+3bs2LEDZ8+eRfXq1RWZMaXX67F9+3YkJyejf//+8PT0xJUrV+Dl5WW65SgH8zGgRcUJV1dXzJkzB6+99ppsNUuDPTc26P4fMEII3Lp1C25ubvj6669lr2feLRwWFoYLFy7g9OnTqFu3riKL0eXn55sGF//+++949tlnAQCPPvqoIj8EyLZpMb5ADWr3FD2Ikquhm0+1r8g8PT3h4+MDHx8feHt7w8HBAY888ojsdS5evIiuXbsiNTUVubm56Ny5Mzw9PfHJJ5/g7t27Re4VWF4pKSkQQiA4OBgHDhywWMPLyckJ1atXL3bpCyWx58YGxcfHW4Qb4xL+bdq0kW1/Jy21adMGkZGR6N69O7p06YJ9+/ahWbNm2LdvH5577jlcvnxZ6yaSFaloK1pTxfOf//wHO3bswNGjR9G4cWOEh4ejQ4cOCA8PLzQAVw69evWCp6cnFi1aBD8/P9PCjzt27MCQIUNMawlVZAw3Nig1NRX+/v5Fdg+npqaibt26stZ7UHei3Bsvbt++Hb1790Z2djZeffVV0/XffvttnD592mb/Eieiysn4B2hsbCx69uyJRo0aKVqvatWq2L17Nxo2bGixqvWFCxcQGhqKnJwc2WsuW7asxOfVXlOLt6VskHFqaFGLiAUFBck+NfT69esWj/Pz83HixAncuHEDHTt2lLUWAERERCAjIwPZ2dkWPVFDhw7VbJ8Ssm5qjS8gKo8jR45gx44d2L59O6ZPnw57e3vTgOKIiAjZw47BYCjy98Dly5cVW2Nr1KhRFo/z8/ORk5MDJycnuLm5qR5u2HNjg+zs7JCenl4o3Fy8eBGhoaGmWQ5KMhgM+Pe//43g4GC89dZbsl77zp07EEKYgszFixexdu1aNGrUCFFRUbLWItt3//iCs2fPIjg4GKNHj5Z9fAGRHI4ePYrPPvsMX3/9dbFB5GH069cPVapUwYIFC+Dp6Yljx46hWrVq6NmzJ+rWravardqkpCS8+eabGD9+vOo/uxlubIhxOuSsWbPw+uuvFzk11N7eXrEl2e935swZREREyD7It0uXLujTpw/eeOMN3LhxA48++igcHR2RkZGBGTNm4M0335S1Htk2ji8gW3DkyBHTTKk//vgD2dnZaN68OSIjIzFt2jRZa125cgWRkZGwt7dHUlISWrVqhaSkJFStWhU7d+4s9IexkhISEvDKK6/g9OnTqtUEeFvKpmi1iFhxkpOTFVk99PDhw6Zda1evXo0aNWrgyJEj+OGHHzBx4kSGG7Kwa9cu7N692+L7AQACAgLw119/adQqov/x8fHB7du30axZM0REROD1119HeHi4YjPgatWqhcTERHz33Xc4fPgwDAYDBg8ejJdffhmurq6K1CyOvb09rly5ompNgOHGpmg1NdR8AS3gXrhKS0vDhg0bZN8kELi3+7jxvvDmzZvRp08f2NnZoW3btrh48aLs9ci2aTG+gKgsli9frmiYKYqrqytee+011daX+fHHHy0eG39PzJ07F0888YQqbTDH21L0QPcvsGUc+d+xY0e89tprcHCQNyM3bdoUQ4YMQe/evdG4cWNs2rQJ7dq1w6FDh9C9e3eLFTCJrGV8AZG10GLmkp2dncVjSZJMvyemT5+uyGr2JWG4sVEHDx7EqlWrkJqaWmg/D1ufKr169Wr0798fBQUF6NixI3777TcAwIcffoidO3fil19+0biFZE2saXwBkTW4f72z+2cuZWVladQy9TDc2KAVK1YgOjoaXbp0wW+//YYuXbogKSkJ6enp6N27d4X4SzU9PR1paWlo1qyZ6S+CAwcOwMvLS5G9WMi23blzBytWrMChQ4dgMBjQokULTcYXEFkrNWcuGWOFltuhMNzYoKZNm2LYsGEYPny4aYGmoKAgDBs2DDVr1sTkyZNlrRcWFlbkF6kkSXBxcUFISAgGDhxY6v1hSuvcuXNITk5GeHg4XF1dIYSoUHsHkTy+/vprvPLKK0U+N378eNlnohDZKqVnLi1atAgzZ840zVCsX78+Ro8ejSFDhihSryR2Dz6FrE1ycjK6d+8OAHB2doZOp4MkSYiNjcWCBQtkr9e1a1ecP38e7u7uiIyMREREBDw8PJCcnIzWrVsjLS0NnTp1wvr162Wpl5mZiaeffhoNGjTAM888Y5pqPmTIEIwdO1aWGlRxjBgxAj///HOh47GxsYrstUZkq5ScuTRhwgSMGjUKPXr0wKpVq7Bq1Sr06NEDsbGxePfddxWpWRLOlrJBvr6+uHXrFgCgdu3aOHHiBJo0aYIbN24osqx2RkYGxo4diwkTJlgcnzp1Ki5evIjNmzfjvffew5QpU9CzZ8+HrhcbGwtHR0ekpqZarNzZr18/xMbGYvr06Q9dgyqOFStW4MUXX8SPP/6I8PBwAEBMTAzWrFlTaTZkJDKnxcylefPm4auvvsJLL71kOvbss8+iadOmiImJwdSpUxWpWyxBNuell14S06dPF0IIMXXqVFGtWjUxZMgQERAQIHr37i17PS8vL5GUlFToeFJSkvDy8hJCCHHq1Cnh4eEhS70aNWqIxMREIYQQHh4eIjk5WQghxPnz54W7u7ssNahi+e6774SPj484ePCgePPNN0WtWrXEmTNntG4WkSYkSbL4sLOzEzVq1BAvvfSSuHLliiI1vb29xdmzZwsdP3PmjKhSpYoiNUvCnhsbNHfuXNy9excAEBcXB0dHR+zatQt9+vQp1LsiBxcXF+zZswchISEWx/fs2QMXFxcA99YacXZ2lqWeTqcrcg+pjIwM2WpQxfLiiy/i+vXrePLJJ1GtWjXs2LGj0NcrUWVhMBhUr/nKK69g3rx5mDFjhsXxBQsW4OWXX1a9PQw3NsjX19f0uZ2dHd566y3Z93cyFxMTgzfeeAOHDh1C69atIUkSDhw4gIULF+Ltt98GAPz6668ICwuTpV54eDiWLVuGKVOmALg3cNlgMGDatGmyD1om23T/wpJG1atXR1hYGL744gvTsft/2BKRPMy/DyVJwsKFC7F582a0bdsWALBv3z5cunRJ9U0zAc6Wsil2dnYPnC0kSZIiWyJ88803mDt3Ls6cOQMAaNiwIWJiYtC/f38A96biGmdPPaw///wTERERaNmyJbZu3Ypnn30WJ0+eRFZWFnbv3o169eo9dA2ybaUNuZIkYevWrQq3hsi6FBf+i/Iw4d+avw8ZbmxISbOR9uzZgzlz5kAIgTt37qjYKmWkp6dj3rx5FuuWDB8+XPVVLomIbE1kZCQOHz4MvV6Phg0bAgDOnj0Le3t7tGjRwnReRQ7/DDc27vTp04iLi8NPP/2El19+GVOmTEHdunUVqZWXl4dr164Vup+rVD2isrp8+TIkSULt2rW1bgqRZmbMmIHt27dj6dKlptWKr1+/jkGDBuGpp56qFEtqMNzYqCtXruC9997D0qVLERUVhQ8//BCNGzdWpFZSUhJee+017Nmzx+K4+GdRvaI2LXwYS5YsgYeHB55//nmL46tWrUJOTo4im3WS7TIYDJg6dSqmT5+O27dvAwA8PT0xduxYvPPOO4X2vCGq6GrXro3Nmzfjscceszh+4sQJdOnSRba1bvr06YP4+Hh4eXmhT58+JZ6r9rZAHFBsY27evIkPPvgAc+bMQfPmzbFlyxY89dRTitYcOHAgHBwc8PPPP6NmzZqKrxL80UcfYf78+YWOV69eHUOHDmW4IQvvvPMOFi1ahI8++ghPPPEEhBDYvXs3Jk2ahLt37+K///2v1k0kUlV2djauXr1aKNxcu3bNtEaaHKpUqWL6fVClShXZrisL1SefU7l9/PHHwtfXV4SGhop169apVtfNzU2cOnVKtXrOzs4iJSWl0PGUlBTh4uKiWjvINtSsWVOsX7++0PF169aJWrVqadAiIm0NGDBA1K1bV6xatUpcunRJXLp0SaxatUoEBgaK6Oho2esZDAZx4cIFodPpZL92ebHnxob83//9H1xdXRESEoKlS5di6dKlRZ4nd/dfaGgoMjIyZL1mSapXr45jx44hMDDQ4vjRo0fh5+enWjvINmRlZRW5meqjjz5aKXY/Jrrf/PnzMW7cOLzyyivIz88HADg4OGDw4MGK7LUmhED9+vVx8uRJ1K9fX/brlwfDjQ2Jjo7WZOPIjz/+GG+99RY++OADNGnSBI6OjhbPe3l5yVrvxRdfxMiRI+Hp6WlaTn/Hjh0YNWoUXnzxRVlrke1r1qwZ5s6di9mzZ1scnzt3Lpo1a6ZRq4i04+bmhi+++ALTpk1DcnIyhBAICQmBu7u7IvXs7OxQv359ZGZmWk244YBieiDjgMz7g5VQaEBxXl4eBgwYgFWrVsHB4V7+NhgMiI6Oxvz58+Hk5CRrPbJtO3bsQPfu3VG3bl20a9cOkiRhz549uHTpEjZu3Kj4mDQia3Xu3DkkJycjPDwcrq6upp/ZStiwYQM++ugjzJs3T7HJLWXBcEMPtGPHjhKf79ChgyJ1z549i6NHj8LV1RVNmjRBQECAInXItqWmpsLBwQGff/45Tp8+DSEEQkND8e9//xt6vZ5LFVClk5mZiRdeeAHbtm2DJElISkpCcHAwBg8eDG9vb0U2H/bx8UFOTg70ej2cnJzg6upq8bzat4gZbsiqGb88tbgdR7bB3t4eaWlpqF69usXxzMxMVK9eXfaeRSJrFx0djWvXrmHhwoVo1KgRjh49iuDgYGzevBmxsbE4efKk7DXj4+NL/Dmt9ixXjrmhUsvJyUFqairy8vIsjjdt2lT2WsuWLcO0adOQlJQEAGjQoAHGjx+PAQMGyF6LbFtxf5/dvn1blu1AiGzN5s2b8euvv6JOnToWx+vXr4+LFy8qUnPgwIGKXLe8GG7ogf7++28MGjQIv/zyS5HPy/2X8YwZMzBhwgSMGDHCYt2SN954AxkZGYiNjZW1Htkm4/45kiRh4sSJFjvJFxQUYP/+/WjevLlGrSPSjk6ns/h+MMrIyICzs7MiNa2tB5Xhhh5o9OjRuH79Ovbt24fIyEisXbsWV69eNa0KK7c5c+Zg3rx5FjvJ9uzZE4899hgmTZrEcEMAgCNHjgC413Nz/Phxi4HmTk5OaNasGcaNG6dV84g0Ex4ejmXLlmHKlCkA7v0BYDAYMG3atFJvdllWxfWg5ubmajIJhOGGHmjr1q1Yv349WrduDTs7OwQEBKBz587w8vLChx9+iO7du8taLy0tDe3bty90vH379khLS5O1Ftmubdu2AQAGDRqEWbNmyb4kAZGtmjZtGiIiIpCQkIC8vDy89dZbOHnyJLKysrB7925ZaxmXYJAkCQsXLoSHh4fpuYKCAuzcubPIdaiUxnBDD6TT6Uxdjb6+vvj777/RoEEDNGnSBIcPH5a9XkhICL7//nu8/fbbFsdXrlxpNWsokPVYsmSJ1k0gsiqhoaE4duwY5s2bB3t7e+h0OvTp0wfDhw9HzZo1Za01c+ZMAPd6bubPnw97e3vTc05OTggMDCxyOx2lMdzQAzVs2BBnzpxBYGAgmjdvji+//NL0BSv3NwoATJ48Gf369cPOnTvxxBNPQJIk7Nq1C1u2bMH3338vez0ioooiPz8fXbp0wZdffonJkycrXi8lJQUAEBkZiTVr1ph2Idcap4LTA33zzTfIz8/HwIEDceTIEURFRSEjIwNOTk5YunQp+vXrJ3vNw4cPY8aMGTh16pRp3ZKxY8ciLCxM9lpERBVJtWrVsGfPHk17ugsKCnD8+HEEBARoEngYbqjMcnJycPr0adStWxdVq1aV9dr5+fkYOnQoJkyYgODgYFmvTURUGYwdOxaOjo746KOPVKs5evRoNGnSBIMHD0ZBQQHCw8Oxd+9euLm54eeff0ZERIRqbQEYbqgYxmm2pTFjxgxZa3t7e+Pw4cMMN0RE5RATE4Nly5YhJCQErVq1KrSnlNw/swGgdu3aWL9+PVq1aoV169Zh+PDh2LZtG5YtW4Zt27bJPpD5QTjmhopknGb7IEqsHNy7d2+sW7euTAGLiKgyO3bsGBo3bgw7OzucOHECLVq0AHBvGxtzSq32npmZiUceeQQAsHHjRjz//PNo0KABBg8eXGhTWzUw3FCRjNNstRASEoIpU6Zgz549aNmyZaG/OkaOHKlRy4iIrFNYWJhpEb2LFy/i4MGD8PPzU61+jRo18Oeff6JmzZrYtGkTvvjiCwD3hjGYz6BSC8MNWZ2FCxfC29sbhw4dwqFDhyyekySJ4YaI6D7e3t5ISUlB9erVceHCBRgMBlXrDxo0CC+88AJq1qwJSZLQuXNnAMD+/fu5zg0R8L+phQA3ziQiKo2+ffuiQ4cOpnDRqlWrYntMzp8/L3v9SZMmoXHjxrh06RKef/550zYP9vb2+L//+z/Z6z0IBxSTVVq0aBFmzpxp2jizfv36GD16NIYMGaJxy4iIrNOmTZtw7tw5jBw5Eu+//z48PT2LPG/UqFEqt0x97LkhqzNhwgTMnDkTMTExaNeuHQBg7969iI2NxYULFzB16lSNW0hEZH26du0KADh06BBGjRpVbLiRy+zZszF06FC4uLg8cNCw2sMJ2HNDVqdq1aqYM2cOXnrpJYvj3333HWJiYpCRkaFRy4iIyCgoKAgJCQnw8/NDUFBQsedJkqTIrbCSsOeGrE5BQQFatWpV6HjLli2h1+s1aBEREd3PfHyk+efWgD03ZHViYmLg6OhYaKGpcePG4c6dO/j88881ahkRERmVdi0ySZIwffp0hVtjiT03ZJUWLVqEzZs3o23btgCAffv24dKlS4iOjrb4hlJipU0iInqw+xd7PXToEAoKCtCwYUMA9xYQtLe3R8uWLVVvG8MNWR3z1TWTk5MB3NsIrlq1ajhx4oTpPE4PJyLSjvlirzNmzICnpyeWLl1q2ijz+vXrGDRoEJ566inV28bbUkRERPRQateujc2bN+Oxxx6zOH7ixAl06dIFV65cUbU9dqpWIyIiogonOzsbV69eLXT82rVruHXrlurtYbghIiKih9K7d28MGjQIq1evxuXLl3H58mWsXr0agwcPRp8+fVRvD29LERER0UPJycnBuHHjsHjxYuTn5wMAHBwcMHjwYEybNq3QBshKY7ghIiIiWeh0OiQnJ0MIgZCQENVDjRHDDREREVUoHHNDREREFQrDDREREVUoDDdERERUoTDcEFGlJkkS1q1bp3UziEhGDDdEpLhr165h2LBhqFu3LpydnfHII48gKioKe/fu1bppRFQBcW8pIlJc3759kZ+fj6VLlyI4OBhXr17Fli1bkJWVpXXTiKgCYs8NESnqxo0b2LVrFz7++GNERkYiICAAjz/+OOLi4tC9e3cA9zbda9KkCdzd3eHv749///vfuH37tuka8fHx8Pb2xs8//4yGDRvCzc0Nzz33HHQ6HZYuXYrAwED4+PggJiYGBQUFptcFBgZiypQp6N+/Pzw8PFCrVi3MmTOnxPb+9ddf6NevH3x8fODn54eePXviwoULpue3b9+Oxx9/HO7u7vD29sYTTzyBixcvyvumEdFDYbghIkV5eHjAw8MD69atQ25ubpHn2NnZYfbs2Thx4gSWLl2KrVu34q233rI4JycnB7Nnz8aKFSuwadMmbN++HX369MHGjRuxceNGLF++HAsWLMDq1astXjdt2jQ0bdoUhw8fRlxcHGJjY/Hbb78V2Y6cnBxERkbCw8MDO3fuxK5du+Dh4YGuXbsiLy8Per0evXr1QocOHXDs2DHs3bsXQ4cO5Q71RNZGEBEpbPXq1cLHx0e4uLiI9u3bi7i4OHH06NFiz//++++Fn5+f6fGSJUsEAHHu3DnTsWHDhgk3Nzdx69Yt07GoqCgxbNgw0+OAgADRtWtXi2v369dPdOvWzfQYgFi7dq0QQohFixaJhg0bCoPBYHo+NzdXuLq6il9//VVkZmYKAGL79u1lfxOISDXsuSEixfXt2xdXrlzBjz/+iKioKGzfvh0tWrRAfHw8AGDbtm3o3LkzateuDU9PT0RHRyMzMxM6nc50DTc3N9SrV8/0uEaNGggMDISHh4fFsWvXrlnUbteuXaHHp06dKrKdhw4dwrlz5+Dp6WnqcfL19cXdu3eRnJwMX19fDBw4EFFRUejRowdmzZqFtLS0h317iEhmDDdEpAoXFxd07twZEydOxJ49ezBw4EC89957uHjxIp555hk0btwYP/zwAw4dOoTPP/8cAEwb8AGAo6OjxfUkSSrymMFgeGBbiruNZDAY0LJlSyQmJlp8nD17Fv379wcALFmyBHv37kX79u2xcuVKNGjQAPv27SvTe0FEymK4ISJNhIaGQqfTISEhAXq9HtOnT0fbtm3RoEEDXLlyRbY69wePffv24dFHHy3y3BYtWiApKQnVq1dHSEiIxUeVKlVM54WFhSEuLg579uxB48aN8e2338rWXiJ6eAw3RKSozMxMdOzYEV9//TWOHTuGlJQUrFq1Cp988gl69uyJevXqQa/XY86cOTh//jyWL1+O+fPny1Z/9+7d+OSTT3D27Fl8/vnnWLVqFUaNGlXkuS+//DKqVq2Knj174o8//kBKSgp27NiBUaNG4fLly0hJSUFcXBz27t2LixcvYvPmzTh79iwaNWokW3uJ6OFxnRsiUpSHhwfatGmDmTNnIjk5Gfn5+fD398frr7+Ot99+G66urpgxYwY+/vhjxMXFITw8HB9++CGio6NlqT927FgcOnQIkydPhqenJ6ZPn46oqKgiz3Vzc8POnTvxn//8B3369MGtW7dQu3ZtPP300/Dy8sKdO3dw+vRpLF26FJmZmahZsyZGjBiBYcOGydJWIpKHJIQQWjeCiEgJgYGBGD16NEaPHq11U4hIRbwtRURERBUKww0RERFVKLwtRURERBUKe26IiIioQmG4ISIiogqF4YaIiIgqFIYbIiIiqlAYboiIiKhCYbghIiKiCoXhhoiIiCoUhhsiIiKqUBhuiIiIqEL5f2vGzhxLi+opAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example text\n",
    "text = \"Natural language processing is fun. Let's tokenize this text and plot a word frequency distribution.\"\n",
    "\n",
    "# Tokenizing the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Creating the frequency distribution\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# Plotting the frequency distribution\n",
    "fdist.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How do you use SpaCy for dependency parsing of a sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy: nsubj (Head: is)\n",
      "is: ROOT (Head: is)\n",
      "an: det (Head: library)\n",
      "amazing: amod (Head: library)\n",
      "library: attr (Head: is)\n",
      "for: prep (Head: library)\n",
      "NLP: compound (Head: tasks)\n",
      "tasks: pobj (Head: for)\n",
      ".: punct (Head: is)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load SpaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "text = \"SpaCy is an amazing library for NLP tasks.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Displaying the dependency parsing information\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.dep_} (Head: {token.head.text})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How do you use TextBlob for performing text classification based on polarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "# TextBlob can be used to analyze the sentiment polarity of a text and classify it as positive, negative, or neutral.\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample text\n",
    "text = \"I love programming. It's so much fun!\"\n",
    "\n",
    "# Creating a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Classifying polarity\n",
    "if blob.sentiment.polarity > 0:\n",
    "    print(\"Positive sentiment\")\n",
    "elif blob.sentiment.polarity < 0:\n",
    "    print(\"Negative sentiment\")\n",
    "else:\n",
    "    print(\"Neutral sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How do you extract named entities from a text using SpaCy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple (ORG)\n",
      "UK (GPE)\n"
     ]
    }
   ],
   "source": [
    "# SpaCy provides an easy way to extract named entities (such as persons, locations, organizations, etc.) from a given text.\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "text = \"Apple is looking to buy a startup in the UK.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extracting named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How can you calculate TF-IDF scores for a given text using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.49428908 0.         0.         0.49428908 0.2919351\n",
      "  0.2919351  0.58387019]\n",
      " [0.         0.         0.52189879 0.39691748 0.         0.30824183\n",
      "  0.30824183 0.61648367]\n",
      " [0.52189879 0.         0.         0.39691748 0.         0.30824183\n",
      "  0.30824183 0.61648367]]\n",
      "['bird' 'cat' 'dog' 'log' 'mat' 'on' 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"The bird sat on the log.\"\n",
    "]\n",
    "\n",
    "# Creating the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fitting the model and transforming the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Converting the TF-IDF matrix to a dense array and displaying it\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Getting the feature names (words)\n",
    "print(vectorizer.get_feature_names_out())\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How do you create a custom text classifier using NLTK's Naive Bayes classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\anubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 0.7188888888888889\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(words):\n",
    "    return {word: True for word in words}\n",
    "\n",
    "# Preparing the data\n",
    "positive_reviews = [(list(movie_reviews.words(fileid)), 'pos') for fileid in movie_reviews.fileids('pos')]\n",
    "negative_reviews = [(list(movie_reviews.words(fileid)), 'neg') for fileid in movie_reviews.fileids('neg')]\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data = positive_reviews[:100] + negative_reviews[:100]\n",
    "test_data = positive_reviews[100:] + negative_reviews[100:]\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train([(extract_features(words), category) for words, category in train_data])\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Classifier accuracy:\", nltk.classify.util.accuracy(classifier, [(extract_features(words), category) for words, category in test_data]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How do you use a pre-trained model from Hugging Face for text classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef098f34361d49f68bd37fd77ef95b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anubh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anubh\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da336541e3534c63847e214968ee5466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb690bd7aa04d1babe56062d9642d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8527a3cab6542d5900a040c20aa2d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9969214200973511}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained text classification model\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Analyze a sample text\n",
    "result = classifier(\"I love using Hugging Face models for NLP tasks!\")\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. How do you perform text summarization using Hugging Face transformers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c1edae29374a1db7a7076f4fa85282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anubh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anubh\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ab92e37e4a47f0a447fb9d09131744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/sshleifer/distilbart-cnn-12-6/3bac65d18c99463302d12ca75c2220ea714f9c81ce235f205fa818efe71df6ea?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1734697500&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDY5NzUwMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9zc2hsZWlmZXIvZGlzdGlsYmFydC1jbm4tMTItNi8zYmFjNjVkMThjOTk0NjMzMDJkMTJjYTc1YzIyMjBlYTcxNGY5YzgxY2UyMzVmMjA1ZmE4MThlZmU3MWRmNmVhP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=RtPPDZtd7LZjFaZDGnH2UgeR%7EjucBj3PdtLKr9U%7ExNq-LSVFOKs3x538G0Mcur65FryXvXV3FVJYOfWm4g%7EwI%7Eh1jGOhGN2dgsDlpj40jeMSLc8SuyljVT-MiJnFfMbiV-10pT-0GeTy7UzKipc6v7%7EGtuvV-S9DyC1oLAuWvMIYtr50kqF7odKzoADaUu1XnZS-IHB4Bbz3FdU2VS0rKjGfIq4cinSfmDD9u9N23gK%7EzmU3Nn6-3-eUf2qlyzB7mIq7LvPdAckYwEtVrzeBaMwLAkU601KELkqrJwDV2SEBnyr3Tvh0o4RAB%7EkfZMsqLfhXGwvbm1GdtgeUiFTz4A__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f73b6960154f8d9917d8b1910a1efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  27%|##6       | 325M/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/sshleifer/distilbart-cnn-12-6/3bac65d18c99463302d12ca75c2220ea714f9c81ce235f205fa818efe71df6ea?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1734697500&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDY5NzUwMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9zc2hsZWlmZXIvZGlzdGlsYmFydC1jbm4tMTItNi8zYmFjNjVkMThjOTk0NjMzMDJkMTJjYTc1YzIyMjBlYTcxNGY5YzgxY2UyMzVmMjA1ZmE4MThlZmU3MWRmNmVhP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=RtPPDZtd7LZjFaZDGnH2UgeR%7EjucBj3PdtLKr9U%7ExNq-LSVFOKs3x538G0Mcur65FryXvXV3FVJYOfWm4g%7EwI%7Eh1jGOhGN2dgsDlpj40jeMSLc8SuyljVT-MiJnFfMbiV-10pT-0GeTy7UzKipc6v7%7EGtuvV-S9DyC1oLAuWvMIYtr50kqF7odKzoADaUu1XnZS-IHB4Bbz3FdU2VS0rKjGfIq4cinSfmDD9u9N23gK%7EzmU3Nn6-3-eUf2qlyzB7mIq7LvPdAckYwEtVrzeBaMwLAkU601KELkqrJwDV2SEBnyr3Tvh0o4RAB%7EkfZMsqLfhXGwvbm1GdtgeUiFTz4A__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3b88daba674b5e872a9ecb0d9bafa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  27%|##6       | 325M/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/sshleifer/distilbart-cnn-12-6/3bac65d18c99463302d12ca75c2220ea714f9c81ce235f205fa818efe71df6ea?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1734697500&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNDY5NzUwMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9zc2hsZWlmZXIvZGlzdGlsYmFydC1jbm4tMTItNi8zYmFjNjVkMThjOTk0NjMzMDJkMTJjYTc1YzIyMjBlYTcxNGY5YzgxY2UyMzVmMjA1ZmE4MThlZmU3MWRmNmVhP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=RtPPDZtd7LZjFaZDGnH2UgeR%7EjucBj3PdtLKr9U%7ExNq-LSVFOKs3x538G0Mcur65FryXvXV3FVJYOfWm4g%7EwI%7Eh1jGOhGN2dgsDlpj40jeMSLc8SuyljVT-MiJnFfMbiV-10pT-0GeTy7UzKipc6v7%7EGtuvV-S9DyC1oLAuWvMIYtr50kqF7odKzoADaUu1XnZS-IHB4Bbz3FdU2VS0rKjGfIq4cinSfmDD9u9N23gK%7EzmU3Nn6-3-eUf2qlyzB7mIq7LvPdAckYwEtVrzeBaMwLAkU601KELkqrJwDV2SEBnyr3Tvh0o4RAB%7EkfZMsqLfhXGwvbm1GdtgeUiFTz4A__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e3447247d343f1bb8d3e6a01284fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  27%|##7       | 336M/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7559fb111b1460897855bd38941395c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecc9df2a763427399573998afc394bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2cd5fea9ce4a31bfd24eabbe062287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 142, but your input_length is only 55. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face is a company that provides an easy-to-use interface for various NLP tasks . They have a library called transformers that allows you to use pre-trained models for tasks such as text classification, summarization, and translation . The company also has a library of transformers .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained summarization model\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Sample text to summarize\n",
    "text = \"\"\"\n",
    "Hugging Face is a company that provides an easy-to-use interface for various NLP tasks. They have a library called transformers that allows you to use pre-trained models for tasks such as text classification, summarization, and translation.\n",
    "\"\"\"\n",
    "\n",
    "# Generate the summary\n",
    "summary = summarizer(text)\n",
    "\n",
    "# Print the summary\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. How can you create a simple RNN for text classification using Keras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7074 - accuracy: 0.3333\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6489 - accuracy: 0.6667\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5976 - accuracy: 0.6667\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5518 - accuracy: 0.6667\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5109 - accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23ea5fa2590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np   \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"Deep learning is amazing\", \"Natural language processing is fun\"]\n",
    "labels = [1, 1, 0]  # 1: positive, 0: negative\n",
    "\n",
    "# Tokenizing the text\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=10)\n",
    "\n",
    "# Convert labels to a NumPy array\n",
    "y = np.array(labels)\n",
    "\n",
    "# Building the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=64, input_length=10))\n",
    "model.add(SimpleRNN(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiling and training the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. How do you train a Bidirectional LSTM for text classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.6886 - accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6790 - accuracy: 0.6667\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6696 - accuracy: 0.6667\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6603 - accuracy: 0.6667\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6510 - accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23ea8db2c50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np   \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"Deep learning is amazing\", \"Natural language processing is fun\"]\n",
    "labels = [1, 1, 0]  # 1: positive, 0: negative\n",
    "\n",
    "# Tokenizing the text\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=10)\n",
    "\n",
    "# Convert labels to a NumPy array\n",
    "y = np.array(labels)\n",
    "\n",
    "# Building the Bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=64, input_length=10))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiling and training the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. How do you implement GRU (Gated Recurrent Unit) for text classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.6929 - accuracy: 0.6667\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6839 - accuracy: 0.6667\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6751 - accuracy: 0.6667\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6662 - accuracy: 0.6667\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6573 - accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23ead5b6650>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love machine learning\", \"Deep learning is amazing\", \"Natural language processing is fun\"]\n",
    "labels = [1, 1, 0]  # 1: positive, 0: negative\n",
    "\n",
    "# Tokenizing the text\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=10)\n",
    "\n",
    "# Convert labels to a NumPy array\n",
    "y = np.array(labels)\n",
    "\n",
    "# Building the GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=64, input_length=10))\n",
    "model.add(GRU(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compiling and training the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. How do you implement a text generation model using LSTM with Keras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 5s 5s/step - loss: 2.0936\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.0583\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.0255\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 1.9951\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9669\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9409\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.9167\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.8943\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.8735\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.8541\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.8361\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.8192\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.8033\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.7883\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.7741\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7603\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.7470\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.7340\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.7212\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.7085\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.6957\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.6829\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6700\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.6570\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.6438\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.6304\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.6167\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6028\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5886\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5741\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.5592\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.5440\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5285\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.5125\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4961\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4793\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.4619\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4440\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.4254\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.4063\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3866\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.3664\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.3457\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3246\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.3030\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2811\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2588\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2361\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2130\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.1897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23eb2416f10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Sample text\n",
    "text = \"hello world\"\n",
    "\n",
    "# Preparing the dataset (example for character-based text generation)\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Creating input/output pairs\n",
    "X = []\n",
    "y = []\n",
    "seq_length = 3\n",
    "for i in range(len(text) - seq_length):\n",
    "    X.append([char_to_int[char] for char in text[i:i+seq_length]])\n",
    "    y.append(char_to_int[text[i+seq_length]])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# One-hot encoding the labels\n",
    "y = to_categorical(y, num_classes=len(chars))\n",
    "\n",
    "# Reshaping X to be 3D [samples, time steps, features]\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Add a feature dimension\n",
    "\n",
    "# Building the LSTM model for text generation\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2])))  # Adjusted input shape\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Training the model\n",
    "model.fit(X, y, epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. How do you implement a simple Bi-directional GRU for sequence labeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (2, 5)\n",
      "Shape of y: (2, 5, 4)\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 8s 8s/step - loss: 1.1143 - accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1066 - accuracy: 0.2000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0990 - accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0914 - accuracy: 0.7000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0838 - accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23eb7f87750>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, GRU, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Sample data for sequence labeling (NER)\n",
    "texts = [\"John lives in New York\", \"Mary works at Google\"]\n",
    "labels = [[1, 0, 2, 2], [1, 0, 3, 3]]  # 1: person, 0: O, 2: location, 3: organization\n",
    "\n",
    "# Tokenizing the text\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=5)  # Padding to maxlen of 5\n",
    "\n",
    "# One-hot encoding the labels  \n",
    "y = [to_categorical(label, num_classes=4) for label in labels]  # One-hot encode labels\n",
    "y = np.array([np.pad(label, ((0, X.shape[1] - len(label)), (0, 0)), 'constant') for label in y])  # Pad labels to match maxlen of 5\n",
    "\n",
    "# Ensure y has the correct shape: (batch_size, timesteps, num_classes)\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# Building the Bi-directional GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=64, input_length=5))\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True)))  # GRU layer returns sequences for sequence labeling\n",
    "model.add(Dense(4, activation='softmax'))  # Softmax activation for multi-class classification at each timestep\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X, y, epochs=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
