{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention based Models and Transfer Learning\n",
    "\n",
    "\n",
    "\n",
    "### 1. What is BERT and how does it work? \n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. BERT's key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling.\n",
    "\n",
    "### 2. What are the main advantages of using the attention mechanism in neural networks? \n",
    "\n",
    "The main advantages of using the attention mechanism in neural networks include:\n",
    "\n",
    "*   Improved performance on long sequences: The attention mechanism can effectively capture long-range dependencies in sequences, which is crucial for tasks like machine translation and text summarization.\n",
    "    \n",
    "*   Better interpretability: Attention weights provide insights into which parts of the input sequence are most relevant to the output, making the model's decision-making process more transparent.\n",
    "    \n",
    "*   Increased efficiency: Attention can be parallelized, making it faster than traditional sequential models like RNNs.\n",
    "    \n",
    "\n",
    "### 3. How does the self-attention mechanism differ from traditional attention mechanisms? \n",
    "\n",
    "Traditional attention mechanisms typically focus on aligning different input sequences, such as in machine translation where the attention is on aligning the source and target sentences. Self-attention, on the other hand, focuses on capturing relationships within a single sequence. It allows the model to weigh the importance of different parts of the same input sequence, effectively capturing contextual information.\n",
    "\n",
    "### 4. What is the role of the decoder in a Seq2Seq model?\n",
    "\n",
    "In a Sequence-to-Sequence (Seq2Seq) model, the decoder is responsible for generating the output sequence. It takes the context vector produced by the encoder and generates the output tokens one by one, conditioned on the previously generated tokens and the context vector.\n",
    "\n",
    "### 5. What is the difference between GPT-2 and BERT models?\n",
    "\n",
    "The main difference between GPT-2 and BERT lies in their training objectives and architectures:\n",
    "\n",
    "*   BERT is bidirectional, meaning it learns from both the left and right context of a word during training. GPT-2 is unidirectional, learning only from the left context.\n",
    "    \n",
    "*   BERT is primarily designed for tasks that require understanding the relationship between words in a sentence, such as question answering and sentence classification. GPT-2 excels at text generation tasks, where it can produce human-like text.\n",
    "    \n",
    "\n",
    "### 6. Why is the Transformer model considered more efficient than RNNs and LSTMs? \n",
    "\n",
    "The Transformer model is considered more efficient than RNNs and LSTMs mainly due to its ability to parallelize computations. Unlike RNNs, which process sequences sequentially, Transformers can process all input tokens simultaneously, significantly speeding up training and inference.\n",
    "\n",
    "### 7. Explain how the attention mechanism works in a Transformer model. \n",
    "\n",
    "In a Transformer model, the attention mechanism calculates attention weights for each input token in relation to all other tokens in the sequence. These weights determine the importance of each token when generating the output representation for a specific token. The attention mechanism effectively captures relationships between different parts of the input sequence, regardless of their position.\n",
    "\n",
    "### 8. What is the difference between an encoder and a decoder in a Seq2Seq model?\n",
    "\n",
    "In a Seq2Seq model:\n",
    "\n",
    "*   The encoder processes the input sequence and compresses it into a context vector, capturing the essence of the input.\n",
    "    \n",
    "*   The decoder takes the context vector and generates the output sequence, token by token.\n",
    "    \n",
    "\n",
    "### 9. What is the primary purpose of using the self-attention mechanism in transformers?\n",
    "\n",
    "The primary purpose of using the self-attention mechanism in transformers is to capture relationships between different parts of the input sequence. By allowing the model to weigh the importance of different tokens in relation to each other, self-attention helps the model learn contextual representations of the input sequence.\n",
    "\n",
    "### 10. How does the GPT-2 model generate text? \n",
    "\n",
    "The GPT-2 model generates text by predicting the next token in a sequence based on the previously generated tokens. It uses a unidirectional transformer architecture, processing the input sequence from left to right and generating the output token by token.\n",
    "\n",
    "### 11. What is the main difference between the encoder-decoder architecture and a simple neural network? \n",
    "\n",
    "The main difference between the encoder-decoder architecture and a simple neural network is the way they handle sequential data. Simple neural networks typically process inputs and outputs of fixed sizes, while encoder-decoder architectures can handle variable-length input and output sequences, making them suitable for tasks like machine translation and text summarization.\n",
    "\n",
    "### 12. Explain the concept of \"fine-tuning\" in BERT.\n",
    "\n",
    "Fine-tuning in BERT refers to the process of taking a pre-trained BERT model and adapting it to a specific downstream task. This involves adding task-specific layers on top of the BERT model and training the entire network on a labeled dataset for the target task. Fine-tuning allows you to leverage the powerful language representations learned by BERT and apply them to various NLP tasks.\n",
    "\n",
    "### 13. How does the attention mechanism handle long-range dependencies in sequences? \n",
    "\n",
    "The attention mechanism handles long-range dependencies in sequences by directly calculating attention weights between all pairs of tokens in the sequence. Unlike RNNs, which struggle to retain information from distant tokens due to vanishing gradients, attention can effectively capture relationships between tokens regardless of their distance in the sequence.\n",
    "\n",
    "### 14. What is the core principle behind the Transformer architecture? \n",
    "\n",
    "The core principle behind the Transformer architecture is the self-attention mechanism. By allowing the model to weigh the importance of different parts of the input sequence in relation to each other, Transformers can effectively capture contextual information and long-range dependencies.\n",
    "\n",
    "### 15. What is the role of the \"position encoding\" in a Transformer model? \n",
    "\n",
    "Since Transformers don't have a sequential structure like RNNs, position encoding is used to provide information about the position of each token in the input sequence. This helps the model understand the order of the tokens and capture positional relationships between them.\n",
    "\n",
    "### 16. How do Transformers use multiple layers of attention? \n",
    "\n",
    "Transformers use multiple layers of attention (multi-head attention) to capture different aspects of the input sequence. Each attention head focuses on different relationships between tokens, allowing the model to learn a richer representation of the input.\n",
    "\n",
    "### 17. What does it mean when a model is described as \"autoregressive\" like GPT-2? \n",
    "\n",
    "An autoregressive model like GPT-2 predicts the next token in a sequence based on the previously generated tokens. It generates the output sequence token by token, with each token's prediction conditioned on the preceding tokens.\n",
    "\n",
    "### 18. How does BERT's bidirectional training improve its performance? \n",
    "\n",
    "BERT's bidirectional training allows it to learn from both the left and right context of a word, capturing richer contextual representations compared to unidirectional models. This enables BERT to better understand the relationships between words in a sentence and achieve better performance on various NLP tasks.\n",
    "\n",
    "### 19. What are the advantages of using the Transformer over RNN-based models in NLP? \n",
    "\n",
    "Advantages of Transformers over RNN-based models:\n",
    "\n",
    "*   Parallelization: Transformers can process input tokens in parallel, leading to faster training and inference.\n",
    "    \n",
    "*   Long-range dependencies: Transformers can effectively capture long-range dependencies in sequences.\n",
    "    \n",
    "*   Better performance: Transformers have achieved state-of-the-art results on various NLP tasks.\n",
    "    \n",
    "\n",
    "### 20. What is the attention mechanism's impact on the performance of models like BERT and GPT-2?\n",
    "\n",
    "The attention mechanism is crucial to the success of models like BERT and GPT-2. It enables them to capture relationships between different parts of the input sequence effectively, leading to significant performance improvements on various NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Practical\n",
    "\n",
    "### 1. How to implement a simple text classification model using LSTM in Keras? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How to generate sequences of text using a Recurrent Neural Network (RNN)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Generate text\n",
    "def generate_text(seed_text, next_words):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "# Example usage\n",
    "generated_text = generate_text(\"This is a\", 10)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How to perform sentiment analysis using a simple CNN model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How to perform Named Entity Recognition (NER) using spacy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the named entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How to implement a simple Seq2Seq model for machine translation using LSTM in Keras? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the encoder\n",
    "encoder_inputs = keras.Input(shape=(None,))\n",
    "encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units=128, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_inputs = keras.Input(shape=(None,))\n",
    "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(units=128, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(units=vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train_encoder, X_train_decoder], y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How to generate text using a pre-trained transformer model (GPT-2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the GPT-2 text generation pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Generate text\n",
    "generated_text = generator(\"This is a\", max_length=50, num_return_sequences=1)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How to apply data augmentation for text in NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data augmentation techniques for text include:\n",
    "\n",
    "Back-translation: Translate the text to another language and then back to the original language.\n",
    "\n",
    "Synonym replacement: Replace words with their synonyms.\n",
    "\n",
    "Random insertion: Insert random words into the text.\n",
    "\n",
    "Random deletion: Delete random words from the text.\n",
    "\n",
    "Random swap: Swap the positions of random words in the text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/anubhav/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick robert brown fox jump over the lazy heel']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "augmented_text = aug.augment(\"The quick brown fox jumps over the lazy dog\")\n",
    "print(augmented_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. How can you add an Attention Mechanism to a Seq2Seq model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adding an attention mechanism to a Seq2Seq model involves calculating attention weights \n",
    "between the decoder's hidden state and the encoder's outputs at each decoding step.\n",
    "These attention weights determine which parts of the input sequence are most relevant for\n",
    "generating the current output token. The attention mechanism can be implemented using \n",
    "various techniques, such as dot product attention or additive attention.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the encoder\n",
    "input_seq = Input(shape=(None, input_dim))  # input_dim is the size of the vocabulary or feature dimension\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(input_seq)\n",
    "\n",
    "# The encoder's final states are used as the initial states for the decoder\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_input = Input(shape=(None, output_dim))  # output_dim is the size of the vocabulary for the decoder\n",
    "decoder_lstm = LSTM(256, return_state=True, return_sequences=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_input, initial_state=encoder_states)\n",
    "\n",
    "# Add the Attention layer\n",
    "attention = Attention()\n",
    "context_vector, attention_weights = attention([decoder_outputs, encoder_outputs], return_attention_scores=True)\n",
    "\n",
    "# Concatenate the context vector with the decoder output to provide more information\n",
    "decoder_combined_context = Concatenate(axis=-1)([decoder_outputs, context_vector])\n",
    "\n",
    "# Add a Dense layer to predict the next token\n",
    "decoder_dense = Dense(output_dim, activation='softmax')\n",
    "decoder_final_output = decoder_dense(decoder_combined_context)\n",
    "\n",
    "# Define the full model\n",
    "model = Model([input_seq, decoder_input], decoder_final_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit([input_seq_train, decoder_input_train], decoder_output_train, epochs=10, batch_size=64)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
